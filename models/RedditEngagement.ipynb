{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedditEngagement.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "G8vQ713RmNxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reddit Community Engagement\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PmKLeDScYznR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Constants and Global Variables"
      ]
    },
    {
      "metadata": {
        "id": "1NM9WQJbYt2d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 54
        },
        "cellView": "code",
        "outputId": "307b6196-bd91-4ced-b2d6-4b654cf435be",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517714698214,
          "user_tz": 480,
          "elapsed": 723,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.google as tf\n",
        "import collections\n",
        "import tempfile\n",
        "import shutil\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "from IPython import display\n",
        "from google3.pyglib import gfile\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Fraction of the read dataframe to use for learning\n",
        "SAMPLE_FRAC=0.99 #@param\n",
        "# Fraction of the ML sample to use for test\n",
        "TEST_FRAC=0.20 #@param\n",
        "\n",
        "\n",
        "# Options: SubredditClassification, MlbSubredditClassification, CommentsRegression, CommentsClassification\n",
        "current_learning_goal = 'SubredditClassification'\n",
        "current_label_col=''\n",
        "current_feature_cols=''\n",
        "\n",
        "EXAMPLE_WEIGHT_COL = 'ExampleWeight'\n",
        "\n",
        "# Add Url and PostId columns so that we can evaluate the quality of our predictions\n",
        "URL_COL = 'Url'\n",
        "REDDIT_POSTURL_COL = 'RedditPostUrl'\n",
        "URL_LIST_COL = 'UrlList'\n",
        "REDDIT_POSTURL_LIST_COL = 'RedditPostUrlList'\n",
        "\n",
        "EMB_DIM_K = 2\n",
        "\n",
        "# Should training runs reuse model directory checkpoints, or restart\n",
        "RESTART_TRAINING=True #@param\n",
        "ENABLE_SAVE_TOFILES=True #@param \n",
        "\n",
        "# Set preferences for outputs, or leave '' empty for defaults\n",
        "OUTPUT_DIR_PREFERENCE='./tfprojects' # if set to '' will create subdir under /tmp\n",
        "RESULTS_DIR_PREFERENCE='' # if set to '' will create subdir under output_dir\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Options for current_label_col: \n",
        "  Subreddit or RedditSubmitter (for SubredditClassification), \n",
        "  SubredditList (for MlbSubredditClassification),\n",
        "  NumCommentersLogScaled (for CommentsRegression), \n",
        "  NumCommentersBin, NumCommentsBin, ScoreBin (for CommentsClassification)\n",
        "\"\"\"\n",
        "\n",
        "def set_columns_for_goal():\n",
        "  \n",
        "  global current_label_col, current_feature_cols\n",
        "  \n",
        "  if current_learning_goal=='SubredditClassification':\n",
        "    current_label_col = 'Subreddit'\n",
        "  elif current_learning_goal=='MlbSubredditClassification':\n",
        "    current_label_col = 'SubredditList'\n",
        "  elif current_learning_goal=='CommentsRegression':\n",
        "    current_label_col = 'NumCommentersLogScaled'\n",
        "  elif current_learning_goal=='CommentsClassification':\n",
        "    current_label_col = 'ScoreBin'\n",
        "  else:\n",
        "    current_label_col = 'Subreddit'\n",
        "\n",
        "\n",
        "\n",
        "  if current_learning_goal==\"SubredditClassification\":\n",
        "    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", EXAMPLE_WEIGHT_COL]\n",
        "  elif current_learning_goal==\"MlbSubredditClassification\":\n",
        "    current_feature_cols = [URL_LIST_COL, REDDIT_POSTURL_LIST_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitterList\", EXAMPLE_WEIGHT_COL] \n",
        "  elif current_learning_goal==\"CommentsRegression\":\n",
        "    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", \"Subreddit\", EXAMPLE_WEIGHT_COL]\n",
        "  elif current_learning_goal==\"CommentsClassification\":\n",
        "    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", \"Subreddit\", EXAMPLE_WEIGHT_COL]\n",
        "  else:\n",
        "    current_feature_cols = [URL_COL, REDDIT_POSTURL_COL,\"Domain\", \"Tags\", \"BOWEntitiesEncoded\", \"RedditSubmitter\", \"Subreddit\", EXAMPLE_WEIGHT_COL]\n",
        "\n",
        "  return\n",
        "\n",
        "set_columns_for_goal()\n",
        "\n",
        "print('Building model with learning goal: %s, using label column: %s, using feature columns: %s' % (current_learning_goal,current_label_col,current_feature_cols))\n",
        " \n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model with learning goal: SubredditClassification, using label column: Subreddit, using feature columns: ['Url', 'RedditPostUrl', 'Domain', 'Tags', 'BOWEntitiesEncoded', 'RedditSubmitter', 'ExampleWeight']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "23UZZozMi_iz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define util functions"
      ]
    },
    {
      "metadata": {
        "id": "zLrd3M2cjELf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 66
        },
        "outputId": "17293404-7e65-4251-ed68-b0eb62a26a4c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517714705291,
          "user_tz": 480,
          "elapsed": 221,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Create temp directories for outputs\n",
        "\"\"\"\n",
        "\n",
        "def create_output_dirs():\n",
        "  if OUTPUT_DIR_PREFERENCE=='':\n",
        "    output_dir = tempfile.mkdtemp()\n",
        "  else:\n",
        "    output_dir = OUTPUT_DIR_PREFERENCE\n",
        "  #run_id = datetime.fromtimestamp(time.time()).strftime('%Y%m%d-%H%M')  \n",
        "  \n",
        "  if RESULTS_DIR_PREFERENCE=='':\n",
        "    results_dir = os.path.join(output_dir, 'results')\n",
        "  else:\n",
        "    results_dir = RESULTS_DIR_PREFERENCE\n",
        "  \n",
        "  tb_log_dir = clean_tb_log_dir(output_dir)\n",
        "  \n",
        "  return (output_dir,results_dir, tb_log_dir)\n",
        "\n",
        "def clean_model_dir():\n",
        "  model_dir = os.path.join(output_dir, 'model')\n",
        "  if RESTART_TRAINING==True:\n",
        "    shutil.rmtree(model_dir, ignore_errors=True)\n",
        "    os.makedirs(model_dir)\n",
        "  else:\n",
        "    os.makedirs(model_dir)\n",
        "  return model_dir\n",
        "\n",
        "def clean_tb_log_dir(output_dir):\n",
        "  tb_log_dir = os.path.join(output_dir, 'tb_log_dir')\n",
        "  shutil.rmtree(tb_log_dir, ignore_errors=True)\n",
        "  os.makedirs(tb_log_dir)\n",
        "  return tb_log_dir\n",
        "\n",
        "  \n",
        "\"\"\"\n",
        "Define some basic file I/O ops\n",
        "\"\"\"\n",
        "\n",
        "def log_dataframe(df, name):\n",
        "  if ENABLE_SAVE_TOFILES:    \n",
        "    df.to_csv(gfile.GFile(os.path.join(results_dir,name+'.csv'), 'w'), encoding='utf-8', index_label='dataframe_idx')\n",
        "\n",
        "def load_raw_dataframe(path):\n",
        "  # Load it into a pandas dataframe\n",
        "  df = pd.read_csv(gfile.Open(path), names=types.keys(), dtype=types, na_values=\"?\", header=0)\n",
        "  print(\"Size of dataframe: \" + str(len(df.index)) + \" records\") \n",
        "  \n",
        "  return df \n",
        "\n",
        "(output_dir, results_dir, tb_log_dir) = create_output_dirs()\n",
        "\n",
        "print(\"Location of output (model etc) files: %s \" % output_dir)\n",
        "print(\"Location of Tensorboard log files: %s \" % tb_log_dir)\n",
        "print(\"Location of results files: %s\" % results_dir)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Location of output (model etc) files: ./tfprojects \n",
            "Location of Tensorboard log files: ./tfprojects/tb_log_dir \n",
            "Location of results files: ./tfprojects/results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qC6s1Lq8a21s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ingest Raw Data"
      ]
    },
    {
      "metadata": {
        "id": "19_mfqZB5NRl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 106
            },
            {
              "item_id": 108
            }
          ],
          "height": 50
        },
        "cellView": "code",
        "outputId": "f319ae76-9642-4b75-ed63-fcab512a0756",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517714853167,
          "user_tz": 480,
          "elapsed": 141111,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from colabtools import bigquery\n",
        "\n",
        "PROJECT_ID = 'dark-cat' #@param\n",
        "\n",
        "bqclient = bigquery.Create(project_id=PROJECT_ID) \n",
        "  \n",
        "def get_bq_data_for_goal():\n",
        "\n",
        "  query = '''\n",
        "  WITH \n",
        "  s1 AS ( -- Create matches btw reddit and gdelt\n",
        "    SELECT \n",
        "      nwr.DocumentHash,\n",
        "      ARRAY_AGG(DISTINCT nwr.Url) AS MatchedUrls,\n",
        "      ARRAY_AGG(DISTINCT nwr.WebResourceHash) AS MatchedNWRs,\n",
        "      ARRAY_AGG(DISTINCT rwr.WebResourceHash) AS MatchedRWRs, \n",
        "      COUNT(*) AS cnt\n",
        "    FROM discussion_opinions.webresource rwr \n",
        "      INNER JOIN news_opinions.webresource nwr ON nwr.Url = rwr.MetaFields[SAFE_OFFSET(0)]\n",
        "    WHERE \n",
        "      rwr.MetaFields[SAFE_OFFSET(0)] <> 'unavailable' -- 0-index metafield contains external URL\n",
        "      AND nwr._PARTITIONTIME BETWEEN TIMESTAMP('2017-06-01') AND TIMESTAMP('2017-09-30') \n",
        "      AND rwr._PARTITIONTIME BETWEEN TIMESTAMP('2017-06-01') AND TIMESTAMP('2017-09-30') \n",
        "    GROUP BY 1\n",
        "    ORDER BY 5 DESC\n",
        "  )\n",
        "  -- SELECT * FROM s1 LIMIT 1000\n",
        "  -- SELECT COUNT(*) FROM s1 -- 200265\n",
        "  , s2a AS (\n",
        "    SELECT nd.DocumentHash, \n",
        "      nd.PublicationTime AS NewsPubTime,   \n",
        "      nd.Author AS NewsAuthor, \n",
        "      ARRAY_AGG(REGEXP_REPLACE(tag.Tag,\"[ | [:punct:]]\",\"_\")) AS TagArray   \n",
        "    FROM s1\n",
        "      INNER JOIN news_opinions.document nd ON nd.DocumentHash = s1.DocumentHash, UNNEST(nd.Tags) AS tag\n",
        "    GROUP BY 1,2,3\n",
        "  )\n",
        "  , s2b AS (\n",
        "    SELECT tag, COUNT(DISTINCT DocumentHash) cnt FROM s2a, UNNEST(s2a.TagArray) AS tag \n",
        "    GROUP BY 1 HAVING COUNT(DISTINCT DocumentHash) >= 3\n",
        "  )\n",
        "  -- SELECT COUNT(*) FROM s2b -- 52264\n",
        "  , s2c AS (\n",
        "    SELECT DocumentHash, tag2 AS Tag\n",
        "    FROM s2a, UNNEST(s2a.TagArray) AS tag2\n",
        "      INNER JOIN s2b ON s2b.tag = tag2\n",
        "  )\n",
        "  , s2 AS (\n",
        "    SELECT s2a.DocumentHash, \n",
        "      s2a.NewsPubTime,   \n",
        "      s2a.NewsAuthor,\n",
        "      ARRAY_TO_STRING(ARRAY_AGG(s2c.Tag),\" \") AS Tags\n",
        "    FROM s2a\n",
        "      LEFT OUTER JOIN s2c ON s2c.DocumentHash = s2a.DocumentHash\n",
        "    GROUP BY 1,2,3\n",
        "  )\n",
        "  -- SELECT COUNT(*) FROM s2 --198657\n",
        "  -- SELECT * FROM s2 LIMIT 1000\n",
        "  , s3 AS (\n",
        "    SELECT s1.DocumentHash, \n",
        "      rwr.Author AS RedditSubmitter,\n",
        "      rwr.PublicationTime AS RedditPubTime,\n",
        "      rwr.MetaFields[SAFE_OFFSET(4)] AS Domain, \n",
        "      rwr.MetaFields[SAFE_OFFSET(0)] AS Url,\n",
        "      rwr.MetaFields[SAFE_OFFSET(1)] AS Subreddit,\n",
        "      rwr.MetaFields[SAFE_OFFSET(2)] AS Score,\n",
        "      rwr.CollectionItemId AS RedditPostId\n",
        "    FROM s1, UNNEST(s1.MatchedRWRs) AS rwrHash\n",
        "      INNER JOIN discussion_opinions.webresource rwr ON rwr.WebResourceHash = rwrHash\n",
        "    GROUP BY 1,2,3,4,5,6,7,8\n",
        "  ),\n",
        "  -- SELECT * FROM s3 LIMIT 1000\n",
        "  -- SELECT COUNT(*) FROM s3 -- 429648\n",
        "  s3aa AS (\n",
        "    SELECT Url FROM s3 GROUP BY 1\n",
        "  ),\n",
        "  s3ab AS (\n",
        "    SELECT gkg.DocumentIdentifier, gkg.V2Themes, gkg.AllNames, gkg.V2Locations\n",
        "    FROM `gdelt-bq.gdeltv2.gkg` gkg \n",
        "      INNER JOIN s3aa ON s3aa.Url = gkg.DocumentIdentifier\n",
        "  )\n",
        "  ,s3ac AS ( -- Mentions of Themes\n",
        "    SELECT s3ab.DocumentIdentifier, SPLIT(theme_mentions,',')[SAFE_OFFSET(0)] AS Entity, SPLIT(theme_mentions,',')[SAFE_OFFSET(1)] AS Offset\n",
        "    FROM s3ab, UNNEST(SPLIT(s3ab.V2Themes,\";\")) AS theme_mentions\n",
        "  )\n",
        "  -- SELECT * FROM s3ac LIMIT 1000\n",
        "  ,s3ad AS (\n",
        "    SELECT s3ab.DocumentIdentifier, \n",
        "      REPLACE(SPLIT(name_mentions,',')[SAFE_OFFSET(0)],' ','_') AS Name, \n",
        "      SPLIT(name_mentions,',')[SAFE_OFFSET(1)] AS Offset\n",
        "    FROM s3ab, UNNEST(SPLIT(s3ab.AllNames,\";\")) AS name_mentions\n",
        "  )\n",
        "  -- SELECT * FROM s3ad LIMIT 1000\n",
        "  ,s3ae AS ( -- Calculate frequency stats for Name mentions\n",
        "    SELECT Name, COUNT(DISTINCT DocumentIdentifier) FROM s3ad \n",
        "    GROUP BY 1 HAVING COUNT(DISTINCT DocumentIdentifier) >= 10\n",
        "  )\n",
        "  -- SELECT * FROM s3ae LIMIT 1000\n",
        "  ,s3af AS (-- Filter mentions of Names\n",
        "    SELECT s3ad.DocumentIdentifier, s3ad.Name AS Entity, s3ad.Offset\n",
        "    FROM s3ad INNER JOIN s3ae ON s3ae.Name = s3ad.Name\n",
        "  )\n",
        "  -- SELECT DISTINCT Entity FROM s3af\n",
        "  ,s3ag AS ( -- Mentions of Locations\n",
        "    SELECT s3ab.DocumentIdentifier, SPLIT(loc_mentions,'#') AS LocFieldArray \n",
        "    FROM s3ab, UNNEST(SPLIT(s3ab.V2Locations,\";\")) AS loc_mentions\n",
        "  )\n",
        "  -- SELECT * FROM s3ag LIMIT 1000\n",
        "  ,s3ah AS (\n",
        "    SELECT \n",
        "      s3ag.DocumentIdentifier, \n",
        "      REPLACE(REPLACE(LocFieldArray[SAFE_OFFSET(1)],' ','_'),',','_') AS Loc, \n",
        "      LocFieldArray[SAFE_OFFSET(8)] AS Offset\n",
        "    FROM s3ag\n",
        "  )\n",
        "  ,s3ai AS ( -- Calculate frequency stats for Location mentions\n",
        "    SELECT Loc, COUNT(DISTINCT DocumentIdentifier) FROM s3ah \n",
        "    GROUP BY 1 HAVING COUNT(DISTINCT DocumentIdentifier) >= 10\n",
        "  )\n",
        "  -- SELECT * FROM s3ae LIMIT 1000\n",
        "  ,s3aj AS ( -- Filter mentions of Locations\n",
        "    SELECT s3ah.DocumentIdentifier, s3ah.Loc AS Entity, s3ah.Offset\n",
        "    FROM s3ah INNER JOIN s3ai ON s3ai.Loc = s3ah.Loc\n",
        "  )\n",
        "  ,s3ak AS ( -- Join all Themes, Locations, Names\n",
        "    SELECT DocumentIdentifier, Entity, Offset FROM s3ac\n",
        "    UNION ALL\n",
        "    SELECT DocumentIdentifier, Entity, Offset FROM s3af\n",
        "    UNION ALL\n",
        "    SELECT DocumentIdentifier, Entity, Offset FROM s3aj\n",
        "  ) \n",
        "  -- SELECT COUNT(DISTINCT Entity) FROM s3ak -- 36412\n",
        "  ,s3an AS ( -- Create Encoding for Entities\n",
        "    SELECT Entity, cnt, CAST(RANK() OVER (ORDER BY cnt DESC, Entity ASC) AS STRING) AS EntityIdx \n",
        "    FROM (SELECT Entity, COUNT(*) AS cnt FROM s3ak GROUP BY 1) \n",
        "  )\n",
        "  -- SELECT * FROM s3an ORDER BY CAST(EntityIdx AS INT64) ASC LIMIT 1000\n",
        "  ,s3a AS (\n",
        "    SELECT DocumentIdentifier, \n",
        "      STRING_AGG(DISTINCT EntityIdx,\" \") AS BOWEntitiesEncoded, -- For Bag-of-Words encoding order is not important\n",
        "      COUNT(DISTINCT s3ak.Entity) AS BOWEncodingLength,\n",
        "      STRING_AGG(DISTINCT s3ak.Entity,\" \") AS EntitiesBOW \n",
        "      -- STRING_AGG(EntityIdx,\" \" ORDER BY Offset ASC) AS EntitiesSeqEncoded, -- For CNN and RNN analysis, use Entity Sequence\n",
        "      -- COUNT(*) AS EntitiesSeqLength,\n",
        "      -- STRING_AGG(s3ak.Entity,\" \" ORDER BY Offset ASC) AS EntitiesSeq\n",
        "    FROM s3ak\n",
        "      INNER JOIN s3an ON s3ak.Entity = s3an.Entity\n",
        "    WHERE s3ak.Entity<>\"\"\n",
        "    GROUP BY 1\n",
        "  )\n",
        "  -- SELECT * FROM s3a LIMIT 1000\n",
        "  -- SELECT COUNT(*) FROM s3a -- 429648\n",
        "  , s3b AS (\n",
        "    SELECT s3.RedditPostId, \n",
        "      COUNT(DISTINCT rwr.Author) AS NumCommenters,\n",
        "      COUNT(*) AS NumComments\n",
        "    FROM s3\n",
        "      INNER JOIN discussion_opinions.webresource rwr ON rwr.MetaFields[SAFE_OFFSET(3)] = s3.RedditPostId\n",
        "    WHERE rwr.Author <> '[deleted]' \n",
        "      AND rwr.ParentWebResourceHash IS NOT NULL -- exclude the actual post item\n",
        "    GROUP BY 1\n",
        "  )\n",
        "  -- SELECT * FROM s3b WHERE NumComments < 10 ORDER BY NumCommenters DESC LIMIT 1000\n",
        "  -- SELECT COUNT(*) FROM s3b -- 419004\n",
        "  , s4 AS (\n",
        "    SELECT s2.*, \n",
        "      s3.Domain, \n",
        "      s3.Url,\n",
        "      CONCAT(\"https://www.reddit.com/r/\",s3.Subreddit,\"/comments/\", SUBSTR(s3.RedditPostId,4), \"/\") AS RedditPostUrl,\n",
        "      s3.RedditSubmitter,\n",
        "      s3.RedditPubTime,\n",
        "      s3.Subreddit,\n",
        "      s3.Score,\n",
        "      IFNULL(s3b.NumCommenters,0) AS NumCommenters,\n",
        "      IFNULL(s3b.NumComments,0) AS NumComments,\n",
        "      TIMESTAMP_DIFF(s3.RedditPubTime, s2.NewsPubTime,  MINUTE) AS PostSubmitDelay,\n",
        "      s3a.BOWEntitiesEncoded,\n",
        "      s3a.BOWEncodingLength,\n",
        "      s3a.EntitiesBOW\n",
        "      -- s3a.EntitiesSeqEncoded,\n",
        "      -- s3a.EntitiesSeqLength,\n",
        "      -- s3a.EntitiesSeq\n",
        "    FROM s2 \n",
        "      INNER JOIN s3 ON s3.DocumentHash = s2.DocumentHash\n",
        "      LEFT OUTER JOIN s3b ON s3b.RedditPostId = s3.RedditPostId\n",
        "      LEFT OUTER JOIN s3a ON s3a.DocumentIdentifier = s3.Url\n",
        "  )\n",
        "  -- SELECT COUNT(*) FROM s4 -- 425548 / 425548\n",
        "  -- SELECT * FROM s4 LIMIT 1000\n",
        "  , s5 AS ( -- Creates a ranking of Subreddits based on frequency of posts\n",
        "    SELECT Subreddit, cnt, RANK() OVER (ORDER BY cnt DESC) AS SubredditRank\n",
        "    FROM (SELECT Subreddit, COUNT(*) AS cnt FROM s4 GROUP BY 1)\n",
        "  )\n",
        "  , s8 AS (\n",
        "    SELECT s4.*\n",
        "      , (CASE WHEN s5.SubredditRank < 200 THEN 1 ELSE 0 END) AS IsTop200Subreddit\n",
        "    FROM s4\n",
        "      INNER JOIN s5 ON s5.Subreddit = s4.Subreddit\n",
        "  )\n",
        "  , s9 AS (\n",
        "    SELECT\n",
        "      s8.DocumentHash,\n",
        "      s8.RedditPostUrl,\n",
        "      s8.Url,\n",
        "      s8.Domain,\n",
        "      s8.RedditSubmitter AS RedditSubmitter,\n",
        "      s8.Subreddit,\n",
        "      s8.Score,\n",
        "      s8.NumCommenters,\n",
        "      s8.NumComments,\n",
        "      s8.Tags,\n",
        "      IFNULL(s8.BOWEntitiesEncoded,\"\") AS BOWEntitiesEncoded,\n",
        "      IFNULL(s8.BOWEncodingLength,0) AS BOWEncodingLength,\n",
        "      IFNULL(s8.EntitiesBOW,\"\") AS EntitiesBOW,\n",
        "      -- IFNULL(s8.EntitiesSeqEncoded,\"\") AS EntitiesSeqEncoded,\n",
        "      -- IFNULL(s8.EntitiesSeqLength,0) AS EntitiesSeqLength,\n",
        "      -- IFNULL(s8.EntitiesSeq,\"\") AS EntitiesSeq,\n",
        "      (CASE WHEN s8.Subreddit LIKE '%auto' OR s8.Subreddit IN ('AutoNewspaper','UMukhasimAutoNews','newsbotbot','TheNewsFeed') THEN 1 ELSE 0 END) AS IsAutoSubreddit,\n",
        "      IsTop200Subreddit\n",
        "    FROM s8\n",
        "    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n",
        "  )\n",
        "  '''\n",
        "\n",
        "\n",
        "  if current_learning_goal == 'SubredditClassification':\n",
        "    query += '''SELECT Url, RedditPostUrl, Domain, RedditSubmitter, Subreddit, Tags, BOWEntitiesEncoded FROM s9 WHERE IsTop200Subreddit = 1 AND s9.IsAutoSubreddit = 0 '''\n",
        "  elif current_learning_goal == 'MlbSubredditClassification':\n",
        "    query += '''\n",
        "  SELECT DocumentHash, Domain, Tags, BOWEntitiesEncoded, \n",
        "    STRING_AGG(DISTINCT Url,\" \") AS UrlList,\n",
        "    STRING_AGG(DISTINCT RedditPostUrl,\" \") AS RedditPostUrlList,\n",
        "    STRING_AGG(DISTINCT RedditSubmitter,\" \") AS RedditSubmitterList, \n",
        "    STRING_AGG(DISTINCT Subreddit,\" \") AS SubredditList,\n",
        "    MAX(Score) AS Score,\n",
        "    SUM(NumCommenters) AS NumCommenters,\n",
        "    SUM(NumComments) AS NumComments\n",
        "  FROM s9 \n",
        "  WHERE IsTop200Subreddit = 1 AND s9.IsAutoSubreddit = 0\n",
        "  GROUP BY 1,2,3,4 '''\n",
        "  else:\n",
        "    query += '''SELECT Url, RedditPostUrl, Domain, RedditSubmitter, Subreddit, Tags, BOWEntitiesEncoded, Score, NumCommenters, NumComments FROM s9'''\n",
        "\n",
        "\n",
        "  df = bigquery.ExecuteQuery(query=query, start_row=0, max_rows = 500000, use_legacy_sql=False)\n",
        "  print(\"Size of reddit set: %s records\" % len(df.index)) \n",
        "\n",
        "  # Dump reddit dataset to CSV for inspection\n",
        "  log_dataframe(df,'010-bq-results')\n",
        "\n",
        "  return df\n",
        "\n",
        "reddit_df = get_bq_data_for_goal()\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting on bqjob_r72d73ee9cfa454e4_000001615ed8a6fe_3 ... (108s) Current status: DONE   "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Size of reddit set: 154931 records\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jcxm_vDpmGIB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Transform Raw Data into Feature Columns & Input Functions\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Bf3M1cDCmtvD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 50
        },
        "cellView": "code",
        "outputId": "3f67ab6a-c583-451a-9851-9e245e7cb1e8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517714910825,
          "user_tz": 480,
          "elapsed": 15960,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def get_train_test_sets(fullset_df, train_size, test_size, seed=None):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    seed: The random seed to use when shuffling the data. `None` generates a\n",
        "      unique shuffle every run.\n",
        "  Returns:\n",
        "    a pair of training data, and test data:\n",
        "    `(train, test)`\n",
        "  \"\"\"\n",
        "\n",
        "  # Shuffle the data\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Split the data into train/test subsets.\n",
        "  sample_size = train_size + test_size\n",
        "  sample_df = fullset_df.sample(n=sample_size, random_state=seed)\n",
        "  train = sample_df.head(train_size)\n",
        "  test = sample_df.drop(train.index)\n",
        "\n",
        "  return (train, test)\n",
        "\n",
        "  \n",
        "# Split the Dataframe\n",
        "def split_features_labels(raw_df, feature_cols, label_col):\n",
        "  \n",
        "  features=pd.DataFrame({k: raw_df[k].values for k in feature_cols})\n",
        "  labels=pd.Series(raw_df[label_col].values)\n",
        "\n",
        "  return (features,labels) \n",
        "\n",
        "def embedding_dims(num_tokens, k=2):\n",
        "  return np.ceil(k * (num_tokens**0.25)).astype(int)\n",
        "\n",
        "\n",
        "def space_tokenizer_fn(iterator):\n",
        "  \n",
        "  for x in iterator:\n",
        "    if x is not None:\n",
        "      yield x.split(\" \")\n",
        "    else:\n",
        "      yield []\n",
        "      \n",
        "    \n",
        "def int_converter_fn(a):\n",
        "  return np.asarray([int(i) for i in a],dtype=int)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Normalization utilities compliments of\n",
        "  https://github.com/google/eng-edu/blob/master/ml/cc/exercises/improving_neural_net_performance.ipynb\n",
        "\"\"\"\n",
        "def linear_scale(series):\n",
        "  min_val = series.min()\n",
        "  max_val = series.max()\n",
        "  scale = (max_val - min_val) / 2.0\n",
        "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n",
        "\n",
        "def log1p_normalize(series):\n",
        "  return series.apply(lambda x:math.log(x+1.0))\n",
        "\n",
        "def log_10_1p_normalize(series):\n",
        "  return series.apply(lambda x:math.log10(x+1.0))\n",
        "\n",
        "def clip(series, clip_to_min, clip_to_max):\n",
        "  return series.apply(lambda x:(\n",
        "    min(max(x, clip_to_min), clip_to_max)))\n",
        "\n",
        "def z_score_normalize(series):\n",
        "  mean = series.mean()\n",
        "  std_dv = series.std()\n",
        "  return series.apply(lambda x:(x - mean) / std_dv)\n",
        "\n",
        "def binary_threshold(series, threshold):\n",
        "  return series.apply(lambda x:(1 if x > threshold else 0))\n",
        "\n",
        "\"\"\"\n",
        "Label Weighting Functions\n",
        "\"\"\"\n",
        "def scorebin_weight(series):\n",
        "  return series.apply(lambda x:(0.5 if x == \"1\" else 1.0)  )\n",
        "\n",
        "def weight_inverse_to_freq(series):\n",
        "  \"\"\"\n",
        "  Will calculate a weight inverse to the frequency of the label class, \n",
        "    making small classes more important than indicated by their frequency.\n",
        "    Uses sqrt so as not to diminish the importance of very large classes.\n",
        "  \"\"\"\n",
        "  val_counts = series.value_counts()\n",
        "  min_val = val_counts.min()\n",
        "\n",
        "  return series.apply(lambda x: ((min_val / float(val_counts[x]))**0.5) )\n",
        "\n",
        "\n",
        "def add_engineered_columns(df, learning_goal):\n",
        "  \n",
        "  # rule of thumb, NN's train best when the input features are roughly on the same scale\n",
        "  \n",
        "  if learning_goal == \"CommentsRegression\" or learning_goal == \"CommentsClassification\":\n",
        "    df[\"Score\"] = df[\"Score\"].astype(int)    \n",
        "    \n",
        "    df[\"NumCommentersClipped\"] = ( clip(df[\"NumCommenters\"],0, (10**4 - 1)) )\n",
        "    df[\"NumCommentsClipped\"] = ( clip(df[\"NumComments\"],0, (10**5 - 1)) )\n",
        "    df[\"ScoreClipped\"] = ( clip(df[\"Score\"],0, (10**4 - 1)) )\n",
        "    \n",
        "    df[\"NumCommentersLogScaled\"] = ( log_10_1p_normalize(df[\"NumCommentersClipped\"]) )\n",
        "    df[\"NumCommentsLogScaled\"] = ( log_10_1p_normalize(df[\"NumCommentsClipped\"]) )\n",
        "    df[\"ScoreLogScaled\"] = ( log_10_1p_normalize(df[\"ScoreClipped\"]) )\n",
        "    \n",
        "    # This will result in following binning: 0: 0, 1: 1-9, 2: 10-99, 3: 100-999, 4: 1000-9999 etc\n",
        "    df[\"NumCommentersBin\"] = (np.ceil(df[\"NumCommentersLogScaled\"]).astype(int).astype(str))\n",
        "    df[\"NumCommentsBin\"] = (np.ceil(df[\"NumCommentsLogScaled\"]).astype(int).astype(str))\n",
        "    df[\"ScoreBin\"] = (np.ceil(df[\"ScoreLogScaled\"]).astype(int).astype(str))\n",
        "  \n",
        "    if current_label_col == \"ScoreBin\":\n",
        "      df[EXAMPLE_WEIGHT_COL] = weight_inverse_to_freq(df[\"ScoreBin\"])\n",
        "    else:\n",
        "      df[EXAMPLE_WEIGHT_COL] = 1.0\n",
        "      \n",
        "  else:\n",
        "    df[EXAMPLE_WEIGHT_COL] = 1.0\n",
        "    \n",
        "  return\n",
        "\n",
        "def convert_series_to_nparray(s):\n",
        "  \"\"\"\n",
        "  Converts a pandas Series to a numpy array\n",
        "  \"\"\"\n",
        "  nparray = s.values.astype(type(s[0]))\n",
        "  return nparray\n",
        "\n",
        "\n",
        "def create_train_test_features_labels():\n",
        "\n",
        "  # Add Engineered Columns\n",
        "  add_engineered_columns(reddit_df,learning_goal = current_learning_goal)\n",
        "\n",
        "\n",
        "  # Log results to file\n",
        "  log_dataframe(reddit_df,'030-with-engineered-cols')\n",
        "\n",
        "  reddit_size=len(reddit_df.index)\n",
        "  sample_size=int(np.floor(reddit_size * SAMPLE_FRAC))\n",
        "  test_size=int(np.floor(sample_size * TEST_FRAC))\n",
        "  train_size=sample_size - test_size\n",
        "\n",
        "  (train, test) = get_train_test_sets(fullset_df=reddit_df, train_size=train_size, test_size=test_size,seed=3)\n",
        "  print(\"Size of train set: %d records\" %len(train.index)) \n",
        "  print(\"Size of test set: %d records\" % len(test.index)) \n",
        "\n",
        "  # Log results to file\n",
        "  log_dataframe(train,'040-train')\n",
        "  log_dataframe(test,'050-test')\n",
        "\n",
        "\n",
        "  # Create training and validation splits\n",
        "  training_features, training_labels = split_features_labels(raw_df = train, feature_cols=current_feature_cols, label_col=current_label_col)\n",
        "  validation_features, validation_labels = split_features_labels(raw_df = test, feature_cols=current_feature_cols, label_col=current_label_col)\n",
        "\n",
        "  return (training_features, training_labels,validation_features, validation_labels)\n",
        "\n",
        "(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train set: 122705 records\n",
            "Size of test set: 30676 records\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xd7Lha3VikJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras Multi-Label Model"
      ]
    },
    {
      "metadata": {
        "id": "dXNlcAFhixiW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Keras Multi-Label Model: Tags -> Subreddit\n",
        "\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Flatten, Dense, Dropout, Embedding, Activation, LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "import keras.utils\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "import bisect\n",
        "from sklearn import metrics\n",
        "from keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# sso start: replaced tensorflow.python with tensorflow.google\n",
        "#from colabtools import adhoc_import\n",
        "#with adhoc_import.Google3():\n",
        "#  from google3.third_party.tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n",
        "  \n",
        "# for text embeddings using bag of Word https://cloud.google.com/blog/big-data/2017/10/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts\n",
        "# for sequences: \n",
        "\n",
        "BATCH_SIZE = 50  \n",
        "VALIDATION_SPLIT = 0.1\n",
        "current_epochs = 2\n",
        "\n",
        "# Hidden Layers configuration\n",
        "HIDDEN_UNITS_L1 = 500\n",
        "DROPOUT_L1 = 0.3\n",
        "\n",
        "TAGS_MAX_SEQUENCE_LENGTH = 7\n",
        "TAGS_MAX_NUM_TOKENS=None # don't limit Tags dictionary \n",
        "\n",
        "ENTITIES_MAX_SEQUENCE_LENGTH = 100\n",
        "ENTITIES_MAX_NUM_TOKENS=None # don't limit Entities dictionary \n",
        "\n",
        "\n",
        "def create_bow_input(train, test, max_sequence_length, max_num_tokens=None):\n",
        "\n",
        "  \"\"\"\n",
        "  Args\n",
        "    max_num_tokens: if None, will use the entire dictionary of tokens found\n",
        "  \"\"\"\n",
        "  \n",
        "  tokenizer = Tokenizer(num_words=max_num_tokens,filters='')\n",
        "  tokenizer.fit_on_texts(train)\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) \n",
        "  # when creating Embedding layer, we will add 1 to input_dims \n",
        "  # to account for the padding 0\n",
        "  actual_num_tokens = vocab_size if max_num_tokens == None else min(max_num_tokens,vocab_size)\n",
        "\n",
        "  train_enc = tokenizer.texts_to_sequences(train)\n",
        "  test_enc = tokenizer.texts_to_sequences(test)\n",
        "\n",
        "  train_enc = pad_sequences(train_enc, maxlen=max_sequence_length, padding='post')\n",
        "  test_enc = pad_sequences(test_enc, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "  inverted_dict = dict([[v,k] for k,v in tokenizer.word_index.items()])\n",
        "  \n",
        "  return (train_enc, test_enc, inverted_dict, actual_num_tokens)\n",
        "\n",
        "def create_bow_embedded_layer(train, test, feature_key, max_sequence_length, max_num_tokens ):\n",
        "  \n",
        "  \"\"\"\n",
        "    Args:\n",
        "      train, test - string columns that need to be encoded with integers\n",
        "  \"\"\"\n",
        "  \n",
        "  (train_enc, test_enc, inverted_dict, actual_num_tokens) = create_bow_input(\n",
        "      train=train,\n",
        "      test=test,\n",
        "      max_sequence_length=max_sequence_length,\n",
        "      max_num_tokens=max_num_tokens)\n",
        "\n",
        "  print(\"Using %d unique values for %s\" % (actual_num_tokens,feature_key))\n",
        "\n",
        "  # the very first input layer for the feature\n",
        "  input_layer = Input(shape=(train_enc.shape[1],), name=feature_key)\n",
        "\n",
        "  # Embedding layer \n",
        "  # When embedding, use (tags_actual_num_tokens +1) to account for the padding 0\n",
        "  num_embedding_dims= embedding_dims(actual_num_tokens,EMB_DIM_K)\n",
        "  embedding_layer = Embedding(\n",
        "      input_dim = (actual_num_tokens +1), # to account for the padding 0\n",
        "      output_dim = num_embedding_dims, \n",
        "      input_length=max_sequence_length,\n",
        "      #mask_zero=True\n",
        "      )(input_layer)\n",
        "\n",
        "  # Adding LSTM layer will work best on sequential data. An LSTM will transform \n",
        "  # the vector sequence into a single vector, containing information about the \n",
        "  # entire sequence\n",
        "  # lstm_layer = LSTM(32)(embedding_layer)\n",
        "  # lstm_layer = LSTM(64,return_sequences=True)(embedding_layer)\n",
        "\n",
        "  # Flatten on top of a Embedding will create a bag-of-words matrix\n",
        "  bagofwords_layer = Flatten()(embedding_layer)\n",
        "  \n",
        "  return (train_enc, test_enc, inverted_dict, actual_num_tokens, input_layer, bagofwords_layer)\n",
        "\n",
        "\n",
        "def create_categorical_label_or_feature(training_nparray, test_nparray, min_frequency=0):\n",
        "\n",
        "  \"\"\"\n",
        "  Use VocabularyProcessor because sklearn LabelEncoder() does not support \n",
        "  unseen values in test dataset\n",
        "  \"\"\"\n",
        "  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
        "      max_document_length = 1, \n",
        "      tokenizer_fn = space_tokenizer_fn,\n",
        "      min_frequency=min_frequency\n",
        "      #vocabulary=tf.contrib.learn.preprocessing.CategoricalVocabulary()\n",
        "  )\n",
        "    \n",
        "  train_enc = vocab_processor.fit_transform(training_nparray)\n",
        "  # transform test set using training encoding. Words not found in train set will be set as unknown\n",
        "  test_enc = vocab_processor.transform(test_nparray)  \n",
        "  \n",
        "  train_enc = np.array(list(train_enc))\n",
        "  test_enc = np.array(list(test_enc))\n",
        "  \n",
        "  # VocabularyProcessor outputs a word-id matrix where word ids start from 1 \n",
        "  # and 0 means 'no word'. We do not have to subtract 1 from the index, because\n",
        "  # keras to_categorical works well with that. We also use 0 to pad sequences.\n",
        "  classes = vocab_processor.vocabulary_._reverse_mapping\n",
        "  num_classes = len(classes)\n",
        "\n",
        "  # convert to one-hot representation\n",
        "  train_enc = keras.utils.to_categorical(train_enc, num_classes=num_classes) \n",
        "  test_enc = keras.utils.to_categorical(test_enc, num_classes=num_classes)\n",
        "  \n",
        "  return (train_enc, test_enc, num_classes, classes)\n",
        "\n",
        "def create_multi_label(training_nparray, test_nparray, max_num_classes=None):\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=max_num_classes,filters='')\n",
        "  tokenizer.fit_on_texts(training_nparray)\n",
        "\n",
        "  num_classes = len(tokenizer.word_index) + 1 # for the 0 index unknown class\n",
        "  actual_num_classes = num_classes if max_num_classes == None else min(max_num_classes,num_classes)\n",
        "\n",
        "  train_enc = tokenizer.texts_to_matrix(training_nparray)\n",
        "  test_enc = tokenizer.texts_to_matrix(test_nparray)\n",
        "\n",
        "  inverted_dict = dict([[v,k] for k,v in tokenizer.word_index.items()])\n",
        "  \n",
        "  return (train_enc, test_enc, actual_num_classes, inverted_dict)\n",
        "  \n",
        "\n",
        "def compile_and_fit_model(inputs = [], outputs=[]):\n",
        "  \n",
        "  # Prepare output: Subreddit\n",
        "  if 'Subreddit' in outputs:\n",
        "    global subreddit_train, subreddit_test, num_subreddit_classes, subreddit_classes\n",
        "    if current_learning_goal==\"SubredditClassification\":\n",
        "      (subreddit_train, subreddit_test, num_subreddit_classes, subreddit_classes) = create_categorical_label_or_feature(\n",
        "        training_nparray = convert_series_to_nparray(training_labels),\n",
        "        test_nparray = convert_series_to_nparray(validation_labels))  \n",
        "\n",
        "    elif current_learning_goal==\"MlbSubredditClassification\":\n",
        "      (subreddit_train, subreddit_test, num_subreddit_classes, subreddit_classes) = create_multi_label(\n",
        "        training_nparray = convert_series_to_nparray(training_labels),\n",
        "        test_nparray = convert_series_to_nparray(validation_labels))  \n",
        "    print('Using %d unique values for subreddit' % num_subreddit_classes)\n",
        "\n",
        "  # Prepare input: Domain\n",
        "  if 'Domain' in inputs:\n",
        "    global domain_train, domain_test, num_domain_classes, domain_classes\n",
        "    (domain_train, domain_test, num_domain_classes, domain_classes) = create_categorical_label_or_feature(\n",
        "        training_nparray = training_features['Domain'],\n",
        "        test_nparray = validation_features['Domain'])  \n",
        "    print(\"Using %d unique values for domain\" % num_domain_classes)\n",
        "    # Input layer for domain\n",
        "    domain_input = Input(shape=(domain_train.shape[1],), name='Domain')\n",
        "\n",
        "  # Prepare input/output: RedditSubmitter\n",
        "\n",
        "  if ('RedditSubmitter' in inputs) or ('RedditSubmitter' in outputs):\n",
        "    global submitter_train, submitter_test, num_submitter_classes, submitter_classes\n",
        "    (submitter_train, submitter_test, num_submitter_classes, submitter_classes) = create_categorical_label_or_feature(\n",
        "        training_nparray = training_features['RedditSubmitter'],\n",
        "        test_nparray = validation_features['RedditSubmitter'],\n",
        "        min_frequency = 4)  \n",
        "    print(\"Using %d unique values for submitter\" % num_submitter_classes)\n",
        "    # Input layer for submitter\n",
        "    if ('RedditSubmitter' in inputs):\n",
        "      submitter_input = Input(shape=(submitter_train.shape[1],), name='RedditSubmitter')\n",
        "\n",
        "  # Prepare input: Tags\n",
        "\n",
        "  #global tags_train, tags_test, tags_inverted_dict, tags_actual_num_tokens, tags_input, tags_bagofwords\n",
        "  if 'Tags' in inputs:\n",
        "    global tags_train, tags_test, tags_inverted_dict, tags_actual_num_tokens, tags_input, tags_bagofwords\n",
        "    (tags_train, tags_test, tags_inverted_dict, tags_actual_num_tokens, tags_input, tags_bagofwords) = create_bow_embedded_layer(\n",
        "        train = training_features['Tags'], \n",
        "        test = validation_features['Tags'], \n",
        "        feature_key = 'Tags', \n",
        "        max_sequence_length = TAGS_MAX_SEQUENCE_LENGTH,\n",
        "        max_num_tokens = TAGS_MAX_NUM_TOKENS)\n",
        "\n",
        "  # Prepare input: GDELT Entities \n",
        "\n",
        "  if 'BOWEntitiesEncoded' in inputs:\n",
        "    global entities_train, entities_test, entities_inverted_dict, entities_actual_num_tokens, entities_input, entities_bagofwords\n",
        "    (entities_train, entities_test, entities_inverted_dict, entities_actual_num_tokens, entities_input, entities_bagofwords) = create_bow_embedded_layer(\n",
        "        train=training_features['BOWEntitiesEncoded'],\n",
        "        test=validation_features['BOWEntitiesEncoded'],\n",
        "        feature_key = 'BOWEntitiesEncoded',\n",
        "        max_sequence_length=ENTITIES_MAX_SEQUENCE_LENGTH,\n",
        "        max_num_tokens=ENTITIES_MAX_NUM_TOKENS)\n",
        "\n",
        "  ##########################  \n",
        "  # Merge input branches and build the model\n",
        "  ##########################\n",
        "\n",
        "  def _create_subreddit_model(model_type='multiclass'):\n",
        "    \"\"\"\n",
        "      Creates Multi-Class Single-Label model using softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    # Bring together the Inputs\n",
        "    input_layers = []\n",
        "    preconcat_layers = []\n",
        "    for s in inputs:\n",
        "      if s=='Tags':\n",
        "        input_layers.append(tags_input)\n",
        "        preconcat_layers.append(tags_bagofwords)\n",
        "      elif s=='BOWEntitiesEncoded':\n",
        "        input_layers.append(entities_input)\n",
        "        preconcat_layers.append(entities_bagofwords)\n",
        "      elif s=='Domain':\n",
        "        input_layers.append(domain_input)\n",
        "        preconcat_layers.append(domain_input)\n",
        "      elif s=='RedditSubmitter':\n",
        "        input_layers.append(submitter_input)\n",
        "        preconcat_layers.append(submitter_input)\n",
        "\n",
        "    if len(preconcat_layers) > 1:\n",
        "      joined_1 = keras.layers.concatenate(preconcat_layers, axis=-1)\n",
        "    elif len(preconcat_layers) == 1:\n",
        "      joined_1 = preconcat_layers[0]\n",
        "    else:\n",
        "      raise ValueError('No valid inputs among %s' % (','.join(inputs)))\n",
        "      \n",
        "    # Connect to Hidden Layers\n",
        "    hidden_1 = Dense(HIDDEN_UNITS_L1, activation='relu')(joined_1)\n",
        "    dropout_1 = Dropout(DROPOUT_L1)(hidden_1)\n",
        "    \n",
        "    if model_type=='multiclass':\n",
        "      output_activation='softmax'\n",
        "      loss_function = 'categorical_crossentropy'\n",
        "    elif model_type=='multilabel':\n",
        "      output_activation='sigmoid'\n",
        "      loss_function = 'binary_crossentropy'\n",
        "    \n",
        "    # Add the outputs\n",
        "    output_layers = []\n",
        "    for s in outputs:\n",
        "      if s=='Subreddit':\n",
        "        layer = Dense(num_subreddit_classes, activation=output_activation, name='subreddit_output')(dropout_1)\n",
        "        output_layers.append(layer)\n",
        "      elif s=='RedditSubmitter':\n",
        "        layer = Dense(num_submitter_classes, activation=output_activation, name='submitter_output')(dropout_1)\n",
        "        output_layers.append(layer)\n",
        "      \n",
        "    model = Model(inputs=input_layers, outputs=output_layers)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=loss_function,\n",
        "                  # loss_weights=[1., 0.2]\n",
        "                  metrics=['accuracy','top_k_categorical_accuracy'])\n",
        "                 \n",
        "\n",
        "    return model\n",
        "  \n",
        "  global x_train, x_test, y_train, y_test\n",
        "  \n",
        "  if current_learning_goal==\"SubredditClassification\":\n",
        "    model = _create_subreddit_model(model_type='multiclass')  \n",
        "  elif current_learning_goal==\"MlbSubredditClassification\":\n",
        "    model = _create_subreddit_model(model_type='multilabel')\n",
        "\n",
        "  x_train = {}\n",
        "  x_test = {}\n",
        "  \n",
        "  for s in inputs:\n",
        "    \n",
        "    if s=='Tags':\n",
        "      x_train['Tags'] = tags_train\n",
        "      x_test['Tags'] = tags_test\n",
        "    elif s=='BOWEntitiesEncoded':\n",
        "      x_train['BOWEntitiesEncoded'] = entities_train\n",
        "      x_test['BOWEntitiesEncoded'] = entities_test\n",
        "    elif s=='Domain':\n",
        "      x_train['Domain'] = domain_train\n",
        "      x_test['Domain'] = domain_test\n",
        "    elif s=='RedditSubmitter':\n",
        "      x_train['RedditSubmitter'] = submitter_train\n",
        "      x_test['RedditSubmitter'] = submitter_test\n",
        "   \n",
        "  y_train = {}\n",
        "  y_test = {}\n",
        "\n",
        "  for s in outputs:\n",
        "    \n",
        "    if s=='Subreddit':\n",
        "      y_train['subreddit_output'] = subreddit_train\n",
        "      y_test['subreddit_output'] = subreddit_test\n",
        "    elif s=='RedditSubmitter':\n",
        "      y_train['submitter_output'] = submitter_train\n",
        "      y_test['submitter_output'] = submitter_test\n",
        "    \n",
        "  \n",
        "  \"\"\"\n",
        "  callbacks = [\n",
        "      keras.callbacks.TensorBoard(\n",
        "          log_dir=tb_log_dir,                  \n",
        "          histogram_freq=1,                      \n",
        "          embeddings_freq=1,                     \n",
        "      )\n",
        "  ]\n",
        "  \"\"\"\n",
        "  history = model.fit(x_train, y_train, \n",
        "            epochs=current_epochs, batch_size=BATCH_SIZE,\n",
        "            verbose=2, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "  score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "  if len(outputs)==1:\n",
        "    print('Test data loss: %.3f; top 1 accuracy: %.3f; top 5 accuracy: %.3f;'%(score[0],score[1],score[2]))\n",
        "  elif len(outputs)==2:\n",
        "    print('Test metrics: total loss: %.3f; output_1 loss: %.3f; output_2 loss: %.3f; output_1 top 1 accuracy: %.3f; output_1 top 5 accuracy: %.3f; output_2 top 1 accuracy: %.3f; output_2 top 5 accuracy: %.3f;'%(score[0],score[1],score[2],score[3],score[4],score[5],score[6]))\n",
        "\n",
        "  return (model, history, x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpFnSPRI83D5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate Multi-Label Metrics"
      ]
    },
    {
      "metadata": {
        "id": "PXtcYY3RrRLI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Multi-Label Accuracy calculation based on https://github.com/suraj-deshmukh/Multi-Label-Image-Classification/blob/master/miml.ipynb\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef, hamming_loss, label_ranking_loss, accuracy_score\n",
        "\n",
        "CASE_TYPE_HEADERS = ['100% TP+TN','50%+ TP', '1-49% TP', '0% TP']\n",
        "\n",
        "\n",
        "def eval_multilabel_metrics(model, x_test, y_true):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "    y_pred = the matrix of predicted labels\n",
        "  \"\"\"\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "  y_pred_probs = np.array(y_pred_probs)\n",
        "\n",
        "  ttl_test_samples = y_true.shape[0] \n",
        "  num_classes = y_true.shape[1]\n",
        "\n",
        "\n",
        "  threshold = np.arange(0.05,0.95,0.05)\n",
        "\n",
        "  acc = []\n",
        "  accuracies = []\n",
        "  best_threshold = np.zeros(y_pred_probs.shape[1])\n",
        "  for i in range(y_pred_probs.shape[1]):\n",
        "      y_prob = np.array(y_pred_probs[:,i])\n",
        "      for j in threshold:\n",
        "          y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
        "          acc.append( matthews_corrcoef(y_true[:,i],y_pred))\n",
        "      acc   = np.array(acc)\n",
        "      index = np.where(acc==acc.max()) \n",
        "      accuracies.append(acc.max()) \n",
        "      best_threshold[i] = threshold[index[0][0]]\n",
        "      acc = []\n",
        "\n",
        "\n",
        "  y_pred = np.array([[1 if y_pred_probs[i,j]>=best_threshold[j] else 0 for j in range(num_classes)] for i in range(ttl_test_samples)])\n",
        "\n",
        "\n",
        "  total_correctly_predicted = len([i for i in range(ttl_test_samples) if (y_true[i]==y_pred[i]).sum() == num_classes])\n",
        "  print('Total correctly predicted: %d out of %d (absolute accuracy: %.3f)' % (total_correctly_predicted,ttl_test_samples, total_correctly_predicted/float(ttl_test_samples)))\n",
        "\n",
        "  acc_score = accuracy_score(y_true,y_pred) #same as above\n",
        "  h_loss = hamming_loss(y_true,y_pred)\n",
        "  r_loss = label_ranking_loss(y_true,y_pred_probs)\n",
        "\n",
        "  print('Multi-label accuracy score: %.3f' % acc_score)\n",
        "  print('Hamming loss: %.3f' % h_loss)\n",
        "  print('Label ranking loss: %.3f' % r_loss)\n",
        "\n",
        "  return (y_pred, acc_score,h_loss,r_loss)\n",
        "\n",
        "\n",
        "\n",
        "def prettyprint_nparray(nparray, col_headers=None, row_headers=None):\n",
        "  df = pd.DataFrame(nparray)\n",
        "  if col_headers is not None:\n",
        "    df.columns = col_headers\n",
        "  if row_headers is not None:\n",
        "    df.index = row_headers\n",
        "  print(df)\n",
        "\n",
        "def get_label_bin_header(bin):\n",
        "  \n",
        "  l_start = (2 ** max((bin-1),0)) + 1\n",
        "  l_end = 2 ** bin\n",
        "  if (l_start >= l_end):\n",
        "    res = '%d'%l_end\n",
        "  else:\n",
        "    res = '%d..%d'%(l_start,l_end)  \n",
        "  return res\n",
        "\n",
        "def gen_label_bin_headers(max_bin):\n",
        "  \n",
        "  res=[]\n",
        "  for i in range(max_bin+1):\n",
        "    res.append(get_label_bin_header(i))\n",
        "  return res\n",
        "    \n",
        "    \n",
        "\n",
        "def calc_multilabel_accuracy_stats(y_true, y_pred):\n",
        "\n",
        "  ttl_test_samples = y_true.shape[0]\n",
        "  num_classes = y_true.shape[1]\n",
        "  \n",
        "  max_num_true_labels = max([y_true[i].sum() for i in range(ttl_test_samples)])\n",
        "  max_bin = np.ceil(np.log2(max_num_true_labels)).astype(int)\n",
        "\n",
        "  stats_num_cases = np.zeros((max_bin+1,4),dtype=int)\n",
        "  stats_num_samples = np.zeros((max_bin+1),dtype=int)\n",
        "  # matrix for indexes in x_test for examples; initialize with np.inf\n",
        "  example_by_casetype_bin = np.full((max_bin+1,4),-1,dtype=int) \n",
        "\n",
        "  for i in range(ttl_test_samples):\n",
        "\n",
        "    num_true_labels = y_true[i].sum()\n",
        "    label_bin = np.ceil(np.log2(num_true_labels)).astype(int)\n",
        "\n",
        "    num_all_matches = (y_true[i]==y_pred[i]).sum() # 1's and 0's need to match - the most stringent condition\n",
        "    num_1_matches = np.array([ 1 if y_true[i][j]==y_pred[i][j] and y_true[i][j] ==1 else 0 for j in range(num_classes) ]).sum() # the 1's match \n",
        "\n",
        "    if (num_all_matches == num_classes):\n",
        "      case_type = 0 # 100% True Positives and 100% True Negatives \n",
        "    elif (num_1_matches/float(num_true_labels) >= 0.5):\n",
        "      case_type = 1\n",
        "    elif (num_1_matches/float(num_true_labels) > 0.0):\n",
        "      case_type = 2\n",
        "    else:\n",
        "      case_type = 3\n",
        "\n",
        "    stats_num_samples[label_bin] += 1\n",
        "    stats_num_cases[label_bin,case_type] += 1\n",
        "    \n",
        "    if example_by_casetype_bin[label_bin,case_type] == -1:\n",
        "      example_by_casetype_bin[label_bin,case_type] = i\n",
        "\n",
        "  stats_num_cases_ratios = stats_num_cases.astype(float)\n",
        "\n",
        "  for i in range(stats_num_cases_ratios.shape[0]):\n",
        "    # stats_num_cases_ratios[i] = stats_num_cases_ratios[i] / stats_num_samples[i].astype(float)\n",
        "    stats_num_cases_ratios[i] = stats_num_cases_ratios[i] / ttl_test_samples\n",
        "\n",
        "  stats_num_cases_ratios = np.around(stats_num_cases_ratios, decimals=3)  \n",
        "  stats_by_casetype = np.sum(stats_num_cases_ratios,0)\n",
        "\n",
        "  return (stats_num_samples, stats_num_cases, stats_num_cases_ratios, stats_by_casetype, example_by_casetype_bin)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_input_values_at(npdict, idx):\n",
        "  \n",
        "  \"\"\"\n",
        "    Args\n",
        "      npdict: dictionary of numpy arrays representing  train or test datasets.\n",
        "        used in multi-input keras functional models\n",
        "    Returns \n",
        "      result_list_nparrays: list of single-element numpy arrays populated by \n",
        "        values from npdict located at the same 'idx' position. \n",
        "        Can be used for model.predict calls on a single data point\n",
        "      result_dict_nparrays: dictionary of single-element numpy arrays\n",
        "      \n",
        "  \"\"\"\n",
        "  result_list_nparrays = [] # TODO: remove the nparray output, as it can cause errors during predictions if keys are sorted differently\n",
        "  result_dict_nparrays = {} # the dictionary preserves the key\n",
        "  for k,v in npdict.items():\n",
        "    v_at_idx = np.array([v[idx]])\n",
        "    result_list_nparrays.append( v_at_idx )\n",
        "    result_dict_nparrays[k] = v_at_idx\n",
        "  return (result_list_nparrays,result_dict_nparrays)\n",
        "\n",
        "\n",
        "def decode_domain(domain_enc_nparray):\n",
        "  \n",
        "  idx = np.argmax(domain_enc_nparray)\n",
        "  domain = domain_classes[idx]\n",
        "  \n",
        "  return domain\n",
        "\n",
        "def decode_submitter(submitter_enc_nparray):\n",
        "  \n",
        "  idx = np.argmax(submitter_enc_nparray)\n",
        "  submitter = submitter_classes[idx]\n",
        "  \n",
        "  return submitter\n",
        "\n",
        "def decode_tags(tags_enc, show_unknown=False):\n",
        "  \n",
        "  \"\"\"\n",
        "    Args:\n",
        "      tags_enc - numpy array with just 1 row\n",
        "  \"\"\"\n",
        "  if show_unknown:\n",
        "    tags_class = lambda idx: '<unknown>' if idx==0 else tags_inverted_dict[idx]\n",
        "  else:\n",
        "    tags_class = lambda idx: '' if idx==0 else tags_inverted_dict[idx]\n",
        "    \n",
        "  #tags_decoded = np.array([tags_class(tags_enc[0,idx]) for idx in range(tags_enc.shape[1])])\n",
        "  #tags_str = np.array_str(tags_decoded)\n",
        "  \n",
        "  tags_str = ''\n",
        "  \n",
        "  for idx in range(tags_enc.shape[1]):\n",
        "    tag = tags_class(tags_enc[0,idx])\n",
        "    tags_str = ''.join([tags_str, ' ' + tag])\n",
        "  \n",
        "  return tags_str\n",
        "\n",
        "def decode_single_input(single_input_dict):\n",
        "  \n",
        "  single_input_str=''\n",
        "  \n",
        "  for k,v in single_input_dict.items():\n",
        "    single_feature_str=''\n",
        "    \n",
        "    if k=='Tags':\n",
        "      single_feature_str = decode_tags(v)\n",
        "    elif k=='Domain':\n",
        "      single_feature_str = decode_domain(v)\n",
        "    elif k=='RedditSubmitter':\n",
        "      single_feature_str = decode_submitter(v)\n",
        "    else:\n",
        "      single_feature_str = 'Decode function not implemented'\n",
        "    \n",
        "    single_input_str = ''.join([single_input_str, ' %s [%s]' % (k,single_feature_str) ])\n",
        "  \n",
        "  return single_input_str\n",
        "\n",
        "def get_class_indeces(nhot_class_array):\n",
        "  class_indeces = np.nonzero(nhot_class_array)[0]\n",
        "  return class_indeces\n",
        "\n",
        "def get_classes(class_indeces, classes_dict):\n",
        "  \n",
        "  res=[]\n",
        "  for i in range(class_indeces.shape[0]):\n",
        "    cl = classes_dict[class_indeces[i]]\n",
        "    res.append(cl)\n",
        "  return res\n",
        "\n",
        "def decode_classes(nhot_class_array, classes_dict):\n",
        "  class_indeces = get_class_indeces(nhot_class_array)\n",
        "  classes = get_classes(class_indeces, classes_dict)\n",
        "  res = ','.join(classes)\n",
        "  return res\n",
        "\n",
        "def print_singlelabel_prediction_samples(y_true, classes_dict,\n",
        "                                        top_prediction_K = 5,\n",
        "                                        top1_pred_tofind = 1,\n",
        "                                        topK_pred_tofind = 1,\n",
        "                                        notintopK_pred_tofind = 1,\n",
        "                                        start_idx=0):\n",
        "\n",
        "  ttl_test_samples = y_true.shape[0]\n",
        "  \n",
        "  str_top1=''\n",
        "  str_topK=''\n",
        "  str_notintopK=''\n",
        "\n",
        "  i = start_idx\n",
        "  all_examples_found = False\n",
        "  \n",
        "  while (not all_examples_found) and i < ttl_test_samples:\n",
        "\n",
        "    (single_input, single_input_dict) = get_input_values_at(x_test, i) # \n",
        "    prediction = model.predict(single_input_dict) # resulting prediction is 2D array with shape (1,1)\n",
        "    prediction = prediction.flatten() # convert to 1D array\n",
        "    \n",
        "    # take K largest elements (might be unsorted)\n",
        "    topKidx = np.argpartition(prediction, -top_prediction_K)[-top_prediction_K:] \n",
        "    # sort first (will be in asc order), then reverse array\n",
        "    topKidx = topKidx[np.argsort(prediction[topKidx])]\n",
        "    topKidx = topKidx[::-1]\n",
        "\n",
        "    str_testcase=''\n",
        "    str_testcase = ''.join([str_testcase,'Test record index #%d '%i]) \n",
        "\n",
        "    pred_input = decode_single_input(single_input_dict)\n",
        "\n",
        "    str_testcase = ''.join([str_testcase,'Prediction input: %s\\n' % pred_input])\n",
        "    url_str = get_url_and_reddit_post(validation_features,i)\n",
        "    str_testcase = ''.join([str_testcase,url_str])\n",
        "\n",
        "                            \n",
        "    # what is the actual label?\n",
        "    #actual_label_idx = np.argmax(y_true[i])\n",
        "    actual_label_idx = get_class_indeces(y_true[i])\n",
        "    actual_label_idx = actual_label_idx[0]\n",
        "    actual_label = classes_dict[actual_label_idx]\n",
        "\n",
        "    # is it in top K predictions?\n",
        "    found_str = ''\n",
        "    num_prediction=top_prediction_K+1 # set it to a +1 value to represent \"not in top K\"\n",
        "\n",
        "    found_in_topK = np.nonzero(topKidx == actual_label_idx)\n",
        "    if (len(found_in_topK[0]) > 0):\n",
        "      num_prediction = found_in_topK[0][0] + 1\n",
        "      found_str = ('[ #' + str(num_prediction) + ' prediction]')  \n",
        "    else:\n",
        "      found_str = ('[ not found among ' + str(top_prediction_K) + ' top predictions]')\n",
        "\n",
        "    # build the string with top K classes and their probabilities \n",
        "    pred_str = ''\n",
        "    for j in range(top_prediction_K):\n",
        "      proba = prediction[topKidx[j]]\n",
        "      pred_class = classes_dict[topKidx[j]]\n",
        "      pred_str = ''.join([pred_str, pred_class + ' (' + '%.2f'%proba + ') '])\n",
        "\n",
        "    str_testcase = ''.join([str_testcase,'Top %d predicted labels: %s\\n'%(top_prediction_K,pred_str)]) \n",
        "    str_testcase = ''.join([str_testcase,'Actual label: %s %s\\n' % (actual_label,found_str)])\n",
        "\n",
        "    if num_prediction == 1:\n",
        "      if top1_pred_tofind > 0:\n",
        "        top1_pred_tofind-=1\n",
        "        str_top1 = ''.join([str_top1,str_testcase,'\\n'])\n",
        "    elif num_prediction <= 5:\n",
        "      if topK_pred_tofind > 0:\n",
        "        topK_pred_tofind-=1  \n",
        "        str_topK = ''.join([str_topK,str_testcase,'\\n'])\n",
        "    else:\n",
        "      if notintopK_pred_tofind > 0:\n",
        "        notintopK_pred_tofind-=1\n",
        "        str_notintopK = ''.join([str_notintopK,str_testcase,'\\n'])\n",
        "\n",
        "    i += 1\n",
        "    all_examples_found = True if (top1_pred_tofind + topK_pred_tofind + notintopK_pred_tofind) == 0 else False\n",
        "\n",
        "  print('Examples of exact matches (Actual Label = Top 1 Predicted Label):\\n%s'%str_top1)\n",
        "  print('Examples of approx. matches (Actual Label in Top %d Predicted Labels):\\n%s'%(top_prediction_K,str_topK))\n",
        "  print('Examples of bad matches (Actual Label not in Top %d predictions):\\n%s'%(top_prediction_K,str_notintopK))\n",
        "\n",
        "  return\n",
        "\n",
        "def plot_metrics(history):\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  \n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  \n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.figure()\n",
        "  \n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.show()\n",
        "  return\n",
        "\n",
        "\n",
        "def get_url_and_reddit_post(df, idx):\n",
        "  \n",
        "  res_str = ''\n",
        "  \n",
        "  if URL_COL in df.columns:\n",
        "    val = df[URL_COL][idx]\n",
        "    res_str = ''.join([res_str, 'News Urls: %s\\n' % val])  \n",
        "  elif URL_LIST_COL in df.columns:\n",
        "    val = df[URL_LIST_COL][idx]\n",
        "    res_str = ''.join([res_str, 'News Urls: %s\\n' % val])  \n",
        "      \n",
        "  if REDDIT_POSTURL_COL in df.columns:\n",
        "    val = df[REDDIT_POSTURL_COL][idx]\n",
        "    res_str = ''.join([res_str, 'Reddit Post Urls: %s\\n' % val])  \n",
        "  elif REDDIT_POSTURL_LIST_COL in df.columns:\n",
        "    val = df[REDDIT_POSTURL_LIST_COL][idx]\n",
        "    res_str = ''.join([res_str, 'Reddit Post Urls: %s\\n' % val])  \n",
        "  \n",
        "  return res_str\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "def print_prediction_and_input(idx, notes, y_true, y_pred, classes_dict):\n",
        "  \n",
        "  print('Test record index #%d %s'%(idx,notes))\n",
        "  print(get_url_and_reddit_post(validation_features,idx))\n",
        "  \n",
        "  (single_input, single_input_dict) = get_input_values_at(x_test, idx)  \n",
        "  pred_input = decode_single_input(single_input_dict)\n",
        "  print('Prediction input: %s' % pred_input)\n",
        "\n",
        "  predicted_classes = decode_classes(y_pred[idx], classes_dict)\n",
        "  print('Predicted classes: %s' % predicted_classes)\n",
        "\n",
        "  true_classes = decode_classes(y_true[idx], classes_dict)\n",
        "  print('True/Actual classes: %s' % true_classes)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjDTPSp39hLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tags to Subreddit Classification"
      ]
    },
    {
      "metadata": {
        "id": "FjLYRPZYaZIq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            },
            {
              "item_id": 6
            }
          ],
          "height": 699
        },
        "outputId": "16fe38ab-57c4-4939-b676-be3ddc47bfef",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517715231528,
          "user_tz": 480,
          "elapsed": 54434,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Singlelabel case\n",
        "\n",
        "#plot_metrics(history)\n",
        "#print(model.summary())\n",
        "\n",
        "# Tags -> Subreddit \n",
        "# Single-Label Classification\n",
        "\n",
        "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['Tags'], outputs=['Subreddit'])\n",
        "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 150 unique values for subreddit\n",
            "Using 41309 unique values for Tags\n",
            "Train on 110434 samples, validate on 12271 samples\n",
            "Epoch 1/2\n",
            " - 22s - loss: 3.7352 - acc: 0.1620 - top_k_categorical_accuracy: 0.4294 - val_loss: 3.3661 - val_acc: 0.2122 - val_top_k_categorical_accuracy: 0.5132\n",
            "Epoch 2/2\n",
            " - 22s - loss: 3.0304 - acc: 0.2657 - top_k_categorical_accuracy: 0.5854 - val_loss: 3.2291 - val_acc: 0.2354 - val_top_k_categorical_accuracy: 0.5502\n",
            "Test data loss: 3.225; top 1 accuracy: 0.230; top 5 accuracy: 0.549;\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG at 0x7f139c0b7550>"
            ],
            "image/svg+xml": "<svg height=\"410pt\" viewBox=\"0.00 0.00 169.00 410.00\" width=\"169pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-406 165,-406 165,4 -4,4\" stroke=\"none\"/>\n<!-- 139716588315856 -->\n<g class=\"node\" id=\"node1\"><title>139716588315856</title>\n<polygon fill=\"none\" points=\"25.5,-365.5 25.5,-401.5 135.5,-401.5 135.5,-365.5 25.5,-365.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-379.8\">Tags: InputLayer</text>\n</g>\n<!-- 139722384042512 -->\n<g class=\"node\" id=\"node2\"><title>139722384042512</title>\n<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 161,-328.5 161,-292.5 0,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-306.8\">embedding_5: Embedding</text>\n</g>\n<!-- 139716588315856&#45;&gt;139722384042512 -->\n<g class=\"edge\" id=\"edge1\"><title>139716588315856-&gt;139722384042512</title>\n<path d=\"M80.5,-365.313C80.5,-357.289 80.5,-347.547 80.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"84.0001,-338.529 80.5,-328.529 77.0001,-338.529 84.0001,-338.529\" stroke=\"black\"/>\n</g>\n<!-- 139722384042128 -->\n<g class=\"node\" id=\"node3\"><title>139722384042128</title>\n<polygon fill=\"none\" points=\"25.5,-219.5 25.5,-255.5 135.5,-255.5 135.5,-219.5 25.5,-219.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">flatten_5: Flatten</text>\n</g>\n<!-- 139722384042512&#45;&gt;139722384042128 -->\n<g class=\"edge\" id=\"edge2\"><title>139722384042512-&gt;139722384042128</title>\n<path d=\"M80.5,-292.313C80.5,-284.289 80.5,-274.547 80.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"84.0001,-265.529 80.5,-255.529 77.0001,-265.529 84.0001,-265.529\" stroke=\"black\"/>\n</g>\n<!-- 139722061599568 -->\n<g class=\"node\" id=\"node4\"><title>139722061599568</title>\n<polygon fill=\"none\" points=\"29.5,-146.5 29.5,-182.5 131.5,-182.5 131.5,-146.5 29.5,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">dense_4: Dense</text>\n</g>\n<!-- 139722384042128&#45;&gt;139722061599568 -->\n<g class=\"edge\" id=\"edge3\"><title>139722384042128-&gt;139722061599568</title>\n<path d=\"M80.5,-219.313C80.5,-211.289 80.5,-201.547 80.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"84.0001,-192.529 80.5,-182.529 77.0001,-192.529 84.0001,-192.529\" stroke=\"black\"/>\n</g>\n<!-- 139722384040336 -->\n<g class=\"node\" id=\"node5\"><title>139722384040336</title>\n<polygon fill=\"none\" points=\"18,-73.5 18,-109.5 143,-109.5 143,-73.5 18,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-87.8\">dropout_4: Dropout</text>\n</g>\n<!-- 139722061599568&#45;&gt;139722384040336 -->\n<g class=\"edge\" id=\"edge4\"><title>139722061599568-&gt;139722384040336</title>\n<path d=\"M80.5,-146.313C80.5,-138.289 80.5,-128.547 80.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"84.0001,-119.529 80.5,-109.529 77.0001,-119.529 84.0001,-119.529\" stroke=\"black\"/>\n</g>\n<!-- 139722074508176 -->\n<g class=\"node\" id=\"node6\"><title>139722074508176</title>\n<polygon fill=\"none\" points=\"6,-0.5 6,-36.5 155,-36.5 155,-0.5 6,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-14.8\">subreddit_output: Dense</text>\n</g>\n<!-- 139722384040336&#45;&gt;139722074508176 -->\n<g class=\"edge\" id=\"edge5\"><title>139722384040336-&gt;139722074508176</title>\n<path d=\"M80.5,-73.3129C80.5,-65.2895 80.5,-55.5475 80.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"84.0001,-46.5288 80.5,-36.5288 77.0001,-46.5289 84.0001,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "7rLH231h92JZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tags to Subreddit Samples"
      ]
    },
    {
      "metadata": {
        "id": "W4ateLd5ztyd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 408
        },
        "outputId": "41df7d04-d9c3-4c85-952c-1c7a88a4c5e6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517717923220,
          "user_tz": 480,
          "elapsed": 702,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_singlelabel_prediction_samples(y_true=y_test['subreddit_output'], \n",
        "                                     classes_dict=subreddit_classes)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Examples of exact matches (Actual Label = Top 1 Predicted Label):\n",
            "Test record index #2 Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [nydailynews.com]\n",
            "News Urls: http://www.nydailynews.com/news/national/russian-hackers-targeted-vote-software-company-officials-report-article-1.3223517\n",
            "Reddit Post Urls: https://www.reddit.com/r/politics/comments/6fh0uw/\n",
            "Top 5 predicted labels: politics (0.16) news (0.11) nyc (0.10) worldnewshub (0.06) nottheonion (0.04) \n",
            "Actual label: politics [ #1 prediction]\n",
            "\n",
            "\n",
            "Examples of approx. matches (Actual Label in Top 5 Predicted Labels):\n",
            "Test record index #0 Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [bloomberg.com]\n",
            "News Urls: https://www.bloomberg.com/news/articles/2017-06-05/bitcoin-mania-infects-japanese-stock-market-s-smaller-listings\n",
            "Reddit Post Urls: https://www.reddit.com/r/BitcoinAll/comments/6fk84k/\n",
            "Top 5 predicted labels: ethernews (0.55) Bitcoin (0.13) CryptoCurrency (0.11) BitcoinAll (0.04) technology (0.03) \n",
            "Actual label: BitcoinAll [ #4 prediction]\n",
            "\n",
            "\n",
            "Examples of bad matches (Actual Label not in Top 5 predictions):\n",
            "Test record index #1 Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [bloomberg.com]\n",
            "News Urls: https://www.bloomberg.com/view/articles/2017-06-27/a-sign-to-go-slow-on-the-15-minimum-wage\n",
            "Reddit Post Urls: https://www.reddit.com/r/economy/comments/6k3y0y/\n",
            "Top 5 predicted labels: politics (0.16) LateStageCapitalism (0.11) SandersForPresident (0.10) progressive (0.07) California (0.07) \n",
            "Actual label: economy [ not found among 5 top predictions]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fj_z6yNK9-sA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BOWEntitiesEncoded to Subreddit Classification"
      ]
    },
    {
      "metadata": {
        "id": "s7MxXCyw6v5h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            }
          ],
          "height": 538
        },
        "outputId": "9331943e-536b-4ee4-f736-6fe2ec19bf88",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517716185493,
          "user_tz": 480,
          "elapsed": 107101,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# BOWEntitiesEncoded -> Subreddit \n",
        "# Single-Label Classification\n",
        "\n",
        "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded'], outputs=['Subreddit'])\n",
        "\n",
        "# SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "print_singlelabel_prediction_samples(y_true=y_test['subreddit_output'], classes_dict=subreddit_classes)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 150 unique values for subreddit\n",
            "Using 32376 unique values for BOWEntitiesEncoded\n",
            "Train on 110434 samples, validate on 12271 samples\n",
            "Epoch 1/2\n",
            " - 40s - loss: 3.5588 - acc: 0.1847 - top_k_categorical_accuracy: 0.4632 - val_loss: 3.1717 - val_acc: 0.2339 - val_top_k_categorical_accuracy: 0.5510\n",
            "Epoch 2/2\n",
            " - 40s - loss: 2.8986 - acc: 0.2718 - top_k_categorical_accuracy: 0.6052 - val_loss: 3.0439 - val_acc: 0.2488 - val_top_k_categorical_accuracy: 0.5875\n",
            "Test data loss: 3.081; top 1 accuracy: 0.244; top 5 accuracy: 0.572;\n",
            "Examples of exact matches (Actual Label = Top 1 Predicted Label):\n",
            "Test record index #2 Prediction input:  BOWEntitiesEncoded [Decode function not implemented]\n",
            "News Urls: http://www.nydailynews.com/news/national/russian-hackers-targeted-vote-software-company-officials-report-article-1.3223517\n",
            "Reddit Post Urls: https://www.reddit.com/r/politics/comments/6fh0uw/\n",
            "Top 5 predicted labels: politics (0.17) The_Donald (0.05) news (0.05) technology (0.05) worldnews (0.03) \n",
            "Actual label: politics [ #1 prediction]\n",
            "\n",
            "\n",
            "Examples of approx. matches (Actual Label in Top 5 Predicted Labels):\n",
            "Test record index #0 Prediction input:  BOWEntitiesEncoded [Decode function not implemented]\n",
            "News Urls: https://www.bloomberg.com/news/articles/2017-06-05/bitcoin-mania-infects-japanese-stock-market-s-smaller-listings\n",
            "Reddit Post Urls: https://www.reddit.com/r/BitcoinAll/comments/6fk84k/\n",
            "Top 5 predicted labels: Bitcoin (0.35) BitcoinAll (0.27) ethernews (0.15) CryptoCurrency (0.11) BTCNews (0.02) \n",
            "Actual label: BitcoinAll [ #2 prediction]\n",
            "\n",
            "\n",
            "Examples of bad matches (Actual Label not in Top 5 predictions):\n",
            "Test record index #1 Prediction input:  BOWEntitiesEncoded [Decode function not implemented]\n",
            "News Urls: https://www.bloomberg.com/view/articles/2017-06-27/a-sign-to-go-slow-on-the-15-minimum-wage\n",
            "Reddit Post Urls: https://www.reddit.com/r/economy/comments/6k3y0y/\n",
            "Top 5 predicted labels: politics (0.23) WayOfTheBern (0.10) SandersForPresident (0.07) Political_Revolution (0.06) The_Donald (0.05) \n",
            "Actual label: economy [ not found among 5 top predictions]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cgn9JoK3-ICf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BOWEntitiesEncoded and Domain to Subreddit Classification"
      ]
    },
    {
      "metadata": {
        "id": "RzscRUJd9aSM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            },
            {
              "item_id": 6
            }
          ],
          "height": 813
        },
        "outputId": "f67aa1f4-71c5-4638-a863-af191015e3a7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517716839509,
          "user_tz": 480,
          "elapsed": 197034,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# BOWEntitiesEncoded, Domain -> Subreddit \n",
        "# Single-Label Classification\n",
        "\n",
        "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain'], outputs=['Subreddit'])\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 150 unique values for subreddit\n",
            "Using 4232 unique values for domain\n",
            "Using 32376 unique values for BOWEntitiesEncoded\n",
            "Train on 110434 samples, validate on 12271 samples\n",
            "Epoch 1/2\n",
            " - 81s - loss: 2.9286 - acc: 0.3027 - top_k_categorical_accuracy: 0.6177 - val_loss: 2.3462 - val_acc: 0.3767 - val_top_k_categorical_accuracy: 0.7358\n",
            "Epoch 2/2\n",
            " - 82s - loss: 2.1104 - acc: 0.4165 - top_k_categorical_accuracy: 0.7787 - val_loss: 2.3042 - val_acc: 0.3702 - val_top_k_categorical_accuracy: 0.7409\n",
            "Test data loss: 2.337; top 1 accuracy: 0.366; top 5 accuracy: 0.739;\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG at 0x7f139ad82110>"
            ],
            "image/svg+xml": "<svg height=\"483pt\" viewBox=\"0.00 0.00 311.50 483.00\" width=\"312pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-479 307.5,-479 307.5,4 -4,4\" stroke=\"none\"/>\n<!-- 139722433437520 -->\n<g class=\"node\" id=\"node1\"><title>139722433437520</title>\n<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 205,-474.5 205,-438.5 0,-438.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-452.8\">BOWEntitiesEncoded: InputLayer</text>\n</g>\n<!-- 139722433437008 -->\n<g class=\"node\" id=\"node2\"><title>139722433437008</title>\n<polygon fill=\"none\" points=\"22,-365.5 22,-401.5 183,-401.5 183,-365.5 22,-365.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-379.8\">embedding_7: Embedding</text>\n</g>\n<!-- 139722433437520&#45;&gt;139722433437008 -->\n<g class=\"edge\" id=\"edge1\"><title>139722433437520-&gt;139722433437008</title>\n<path d=\"M102.5,-438.313C102.5,-430.289 102.5,-420.547 102.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"106,-411.529 102.5,-401.529 99.0001,-411.529 106,-411.529\" stroke=\"black\"/>\n</g>\n<!-- 139722433436880 -->\n<g class=\"node\" id=\"node3\"><title>139722433436880</title>\n<polygon fill=\"none\" points=\"47.5,-292.5 47.5,-328.5 157.5,-328.5 157.5,-292.5 47.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-306.8\">flatten_7: Flatten</text>\n</g>\n<!-- 139722433437008&#45;&gt;139722433436880 -->\n<g class=\"edge\" id=\"edge2\"><title>139722433437008-&gt;139722433436880</title>\n<path d=\"M102.5,-365.313C102.5,-357.289 102.5,-347.547 102.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"106,-338.529 102.5,-328.529 99.0001,-338.529 106,-338.529\" stroke=\"black\"/>\n</g>\n<!-- 139722433435472 -->\n<g class=\"node\" id=\"node5\"><title>139722433435472</title>\n<polygon fill=\"none\" points=\"86.5,-219.5 86.5,-255.5 254.5,-255.5 254.5,-219.5 86.5,-219.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-233.8\">concatenate_4: Concatenate</text>\n</g>\n<!-- 139722433436880&#45;&gt;139722433435472 -->\n<g class=\"edge\" id=\"edge3\"><title>139722433436880-&gt;139722433435472</title>\n<path d=\"M118.961,-292.313C127.397,-283.505 137.817,-272.625 147.075,-262.958\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"149.802,-265.172 154.191,-255.529 144.746,-260.33 149.802,-265.172\" stroke=\"black\"/>\n</g>\n<!-- 139722187812496 -->\n<g class=\"node\" id=\"node4\"><title>139722187812496</title>\n<polygon fill=\"none\" points=\"175.5,-292.5 175.5,-328.5 303.5,-328.5 303.5,-292.5 175.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-306.8\">Domain: InputLayer</text>\n</g>\n<!-- 139722187812496&#45;&gt;139722433435472 -->\n<g class=\"edge\" id=\"edge4\"><title>139722187812496-&gt;139722433435472</title>\n<path d=\"M222.797,-292.313C214.237,-283.505 203.664,-272.625 194.269,-262.958\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"196.529,-260.261 187.049,-255.529 191.509,-265.14 196.529,-260.261\" stroke=\"black\"/>\n</g>\n<!-- 139722433434512 -->\n<g class=\"node\" id=\"node6\"><title>139722433434512</title>\n<polygon fill=\"none\" points=\"119.5,-146.5 119.5,-182.5 221.5,-182.5 221.5,-146.5 119.5,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-160.8\">dense_6: Dense</text>\n</g>\n<!-- 139722433435472&#45;&gt;139722433434512 -->\n<g class=\"edge\" id=\"edge5\"><title>139722433435472-&gt;139722433434512</title>\n<path d=\"M170.5,-219.313C170.5,-211.289 170.5,-201.547 170.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"174,-192.529 170.5,-182.529 167,-192.529 174,-192.529\" stroke=\"black\"/>\n</g>\n<!-- 139722303523024 -->\n<g class=\"node\" id=\"node7\"><title>139722303523024</title>\n<polygon fill=\"none\" points=\"108,-73.5 108,-109.5 233,-109.5 233,-73.5 108,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-87.8\">dropout_6: Dropout</text>\n</g>\n<!-- 139722433434512&#45;&gt;139722303523024 -->\n<g class=\"edge\" id=\"edge6\"><title>139722433434512-&gt;139722303523024</title>\n<path d=\"M170.5,-146.313C170.5,-138.289 170.5,-128.547 170.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"174,-119.529 170.5,-109.529 167,-119.529 174,-119.529\" stroke=\"black\"/>\n</g>\n<!-- 139722303522768 -->\n<g class=\"node\" id=\"node8\"><title>139722303522768</title>\n<polygon fill=\"none\" points=\"96,-0.5 96,-36.5 245,-36.5 245,-0.5 96,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-14.8\">subreddit_output: Dense</text>\n</g>\n<!-- 139722303523024&#45;&gt;139722303522768 -->\n<g class=\"edge\" id=\"edge7\"><title>139722303523024-&gt;139722303522768</title>\n<path d=\"M170.5,-73.3129C170.5,-65.2895 170.5,-55.5475 170.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"174,-46.5288 170.5,-36.5288 167,-46.5289 174,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "0_psJ6-d-SoQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BOWEntitiesEncoded and Domain to Subreddit Classification Samples"
      ]
    },
    {
      "metadata": {
        "id": "bNvBG5Pw9e0W",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 408
        },
        "outputId": "194b5dad-10f0-4c19-cd95-2c6c2ef8e1e7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517718141518,
          "user_tz": 480,
          "elapsed": 758,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_singlelabel_prediction_samples(y_true=y_test['subreddit_output'], \n",
        "                                     classes_dict=subreddit_classes,\n",
        "                                     start_idx=10)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Examples of exact matches (Actual Label = Top 1 Predicted Label):\n",
            "Test record index #10 Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [bbc.com]\n",
            "News Urls: http://www.bbc.com/news/world-europe-40242531\n",
            "Reddit Post Urls: https://www.reddit.com/r/worldnews/comments/6gmta5/\n",
            "Top 5 predicted labels: worldnews (0.17) worldnewshub (0.12) europe (0.11) Raytheon (0.09) news (0.07) \n",
            "Actual label: worldnews [ #1 prediction]\n",
            "\n",
            "\n",
            "Examples of approx. matches (Actual Label in Top 5 Predicted Labels):\n",
            "Test record index #15 Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [itv.com]\n",
            "News Urls: http://www.itv.com/news/2017-06-25/john-mcdonnell-grenfell-tower-dead-murdered-by-political-decisions/\n",
            "Reddit Post Urls: https://www.reddit.com/r/electionReformNews/comments/6jeld5/\n",
            "Top 5 predicted labels: ukpolitics (0.44) unitedkingdom (0.24) LabourUK (0.11) electionReformNews (0.04) europe (0.04) \n",
            "Actual label: electionReformNews [ #4 prediction]\n",
            "\n",
            "\n",
            "Examples of bad matches (Actual Label not in Top 5 predictions):\n",
            "Test record index #13 Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [businessinsider.com]\n",
            "News Urls: http://www.businessinsider.com/evidence-russia-meddled-in-us-election-2017-6\n",
            "Reddit Post Urls: https://www.reddit.com/r/neutralnews/comments/6j97bf/\n",
            "Top 5 predicted labels: worldnews (0.14) worldnewshub (0.09) The_Donald (0.09) politics (0.08) europe (0.06) \n",
            "Actual label: neutralnews [ not found among 5 top predictions]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYeIGFz1-bvt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BOWEntitiesEncoded and Domain to Subreddit and RedditSubmitter Classification"
      ]
    },
    {
      "metadata": {
        "id": "CtMq7mNe--6L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 7
            },
            {
              "item_id": 8
            }
          ],
          "height": 941
        },
        "outputId": "12b13c10-9d86-41d5-b77c-cda613f20be9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517784788149,
          "user_tz": 480,
          "elapsed": 265654,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# BOWEntitiesEncoded, Domain -> Subreddit, RedditSubmitter \n",
        "# Single-Label Classification\n",
        "\n",
        "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain'], outputs=['Subreddit','RedditSubmitter'])\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 150 unique values for subreddit\n",
            "Using 4232 unique values for domain\n",
            "Using 2037 unique values for submitter\n",
            "Using 32376 unique values for BOWEntitiesEncoded\n",
            "Train on 110434 samples, validate on 12271 samples\n",
            "Epoch 1/2\n",
            " - 110s - loss: 7.1004 - subreddit_output_loss: 3.0217 - submitter_output_loss: 4.0787 - subreddit_output_acc: 0.2950 - subreddit_output_top_k_categorical_accuracy: 0.6018 - submitter_output_acc: 0.2738 - submitter_output_top_k_categorical_accuracy: 0.5808 - val_loss: 5.8360 - val_subreddit_output_loss: 2.4223 - val_submitter_output_loss: 3.4137 - val_subreddit_output_acc: 0.3717 - val_subreddit_output_top_k_categorical_accuracy: 0.7217 - val_submitter_output_acc: 0.3318 - val_submitter_output_top_k_categorical_accuracy: 0.6629\n",
            "Epoch 2/2\n",
            " - 112s - loss: 5.3022 - subreddit_output_loss: 2.2118 - submitter_output_loss: 3.0904 - subreddit_output_acc: 0.4054 - subreddit_output_top_k_categorical_accuracy: 0.7584 - submitter_output_acc: 0.3491 - submitter_output_top_k_categorical_accuracy: 0.6817 - val_loss: 5.5011 - val_subreddit_output_loss: 2.3026 - val_submitter_output_loss: 3.1985 - val_subreddit_output_acc: 0.3778 - val_subreddit_output_top_k_categorical_accuracy: 0.7467 - val_submitter_output_acc: 0.3364 - val_submitter_output_top_k_categorical_accuracy: 0.6857\n",
            "Test metrics: total loss: 5.453; output_1 loss: 2.318; output_2 loss: 3.135; output_1 top 1 accuracy: 0.373; output_1 top 5 accuracy: 0.740; output_2 top 1 accuracy: 0.334; output_2 top 5 accuracy: 0.691;\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG at 0x7f13a14c12d0>"
            ],
            "image/svg+xml": "<svg height=\"553pt\" viewBox=\"0.00 0.00 626.00 553.00\" width=\"626pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 549)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-549 622,-549 622,4 -4,4\" stroke=\"none\"/>\n<!-- 139722414774096 -->\n<g class=\"node\" id=\"node1\"><title>139722414774096</title>\n<polygon fill=\"none\" points=\"0,-498.5 0,-544.5 343,-544.5 343,-498.5 0,-498.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-517.8\">BOWEntitiesEncoded: InputLayer</text>\n<polyline fill=\"none\" points=\"205,-498.5 205,-544.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"205,-521.5 260,-521.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"260,-498.5 260,-544.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-529.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"260,-521.5 343,-521.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-506.3\">(None, 100)</text>\n</g>\n<!-- 139722414774352 -->\n<g class=\"node\" id=\"node2\"><title>139722414774352</title>\n<polygon fill=\"none\" points=\"11.5,-415.5 11.5,-461.5 331.5,-461.5 331.5,-415.5 11.5,-415.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-434.8\">embedding_8: Embedding</text>\n<polyline fill=\"none\" points=\"172.5,-415.5 172.5,-461.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"172.5,-438.5 227.5,-438.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"227.5,-415.5 227.5,-461.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-446.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"227.5,-438.5 331.5,-438.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-423.3\">(None, 100, 27)</text>\n</g>\n<!-- 139722414774096&#45;&gt;139722414774352 -->\n<g class=\"edge\" id=\"edge1\"><title>139722414774096-&gt;139722414774352</title>\n<path d=\"M171.5,-498.366C171.5,-490.152 171.5,-480.658 171.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"175,-471.607 171.5,-461.607 168,-471.607 175,-471.607\" stroke=\"black\"/>\n</g>\n<!-- 139722414773520 -->\n<g class=\"node\" id=\"node3\"><title>139722414773520</title>\n<polygon fill=\"none\" points=\"37,-332.5 37,-378.5 306,-378.5 306,-332.5 37,-332.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-351.8\">flatten_8: Flatten</text>\n<polyline fill=\"none\" points=\"147,-332.5 147,-378.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"147,-355.5 202,-355.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"202,-332.5 202,-378.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-363.3\">(None, 100, 27)</text>\n<polyline fill=\"none\" points=\"202,-355.5 306,-355.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-340.3\">(None, 2700)</text>\n</g>\n<!-- 139722414774352&#45;&gt;139722414773520 -->\n<g class=\"edge\" id=\"edge2\"><title>139722414774352-&gt;139722414773520</title>\n<path d=\"M171.5,-415.366C171.5,-407.152 171.5,-397.658 171.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"175,-388.607 171.5,-378.607 168,-388.607 175,-388.607\" stroke=\"black\"/>\n</g>\n<!-- 139722414772816 -->\n<g class=\"node\" id=\"node5\"><title>139722414772816</title>\n<polygon fill=\"none\" points=\"114.5,-249.5 114.5,-295.5 516.5,-295.5 516.5,-249.5 114.5,-249.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-268.8\">concatenate_5: Concatenate</text>\n<polyline fill=\"none\" points=\"282.5,-249.5 282.5,-295.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"282.5,-272.5 337.5,-272.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"337.5,-249.5 337.5,-295.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"427\" y=\"-280.3\">[(None, 2700), (None, 4232)]</text>\n<polyline fill=\"none\" points=\"337.5,-272.5 516.5,-272.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"427\" y=\"-257.3\">(None, 6932)</text>\n</g>\n<!-- 139722414773520&#45;&gt;139722414772816 -->\n<g class=\"edge\" id=\"edge3\"><title>139722414773520-&gt;139722414772816</title>\n<path d=\"M210.849,-332.366C228.31,-322.544 249.022,-310.894 267.393,-300.56\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"269.199,-303.56 276.199,-295.607 265.767,-297.459 269.199,-303.56\" stroke=\"black\"/>\n</g>\n<!-- 139722430963536 -->\n<g class=\"node\" id=\"node4\"><title>139722430963536</title>\n<polygon fill=\"none\" points=\"324.5,-332.5 324.5,-378.5 596.5,-378.5 596.5,-332.5 324.5,-332.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388.5\" y=\"-351.8\">Domain: InputLayer</text>\n<polyline fill=\"none\" points=\"452.5,-332.5 452.5,-378.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"480\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"452.5,-355.5 507.5,-355.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"480\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"507.5,-332.5 507.5,-378.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552\" y=\"-363.3\">(None, 4232)</text>\n<polyline fill=\"none\" points=\"507.5,-355.5 596.5,-355.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552\" y=\"-340.3\">(None, 4232)</text>\n</g>\n<!-- 139722430963536&#45;&gt;139722414772816 -->\n<g class=\"edge\" id=\"edge4\"><title>139722430963536-&gt;139722414772816</title>\n<path d=\"M420.878,-332.366C403.296,-322.544 382.44,-310.894 363.941,-300.56\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"365.511,-297.428 355.074,-295.607 362.097,-303.539 365.511,-297.428\" stroke=\"black\"/>\n</g>\n<!-- 139722253461264 -->\n<g class=\"node\" id=\"node6\"><title>139722253461264</title>\n<polygon fill=\"none\" points=\"192.5,-166.5 192.5,-212.5 438.5,-212.5 438.5,-166.5 192.5,-166.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243.5\" y=\"-185.8\">dense_7: Dense</text>\n<polyline fill=\"none\" points=\"294.5,-166.5 294.5,-212.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"294.5,-189.5 349.5,-189.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"349.5,-166.5 349.5,-212.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-197.3\">(None, 6932)</text>\n<polyline fill=\"none\" points=\"349.5,-189.5 438.5,-189.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-174.3\">(None, 500)</text>\n</g>\n<!-- 139722414772816&#45;&gt;139722253461264 -->\n<g class=\"edge\" id=\"edge5\"><title>139722414772816-&gt;139722253461264</title>\n<path d=\"M315.5,-249.366C315.5,-241.152 315.5,-231.658 315.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"319,-222.607 315.5,-212.607 312,-222.607 319,-222.607\" stroke=\"black\"/>\n</g>\n<!-- 139722447297744 -->\n<g class=\"node\" id=\"node7\"><title>139722447297744</title>\n<polygon fill=\"none\" points=\"184,-83.5 184,-129.5 447,-129.5 447,-83.5 184,-83.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-102.8\">dropout_7: Dropout</text>\n<polyline fill=\"none\" points=\"309,-83.5 309,-129.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"309,-106.5 364,-106.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"364,-83.5 364,-129.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405.5\" y=\"-114.3\">(None, 500)</text>\n<polyline fill=\"none\" points=\"364,-106.5 447,-106.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405.5\" y=\"-91.3\">(None, 500)</text>\n</g>\n<!-- 139722253461264&#45;&gt;139722447297744 -->\n<g class=\"edge\" id=\"edge6\"><title>139722253461264-&gt;139722447297744</title>\n<path d=\"M315.5,-166.366C315.5,-158.152 315.5,-148.658 315.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"319,-139.607 315.5,-129.607 312,-139.607 319,-139.607\" stroke=\"black\"/>\n</g>\n<!-- 139722127240080 -->\n<g class=\"node\" id=\"node8\"><title>139722127240080</title>\n<polygon fill=\"none\" points=\"18,-0.5 18,-46.5 305,-46.5 305,-0.5 18,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-19.8\">subreddit_output: Dense</text>\n<polyline fill=\"none\" points=\"167,-0.5 167,-46.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"167,-23.5 222,-23.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"222,-0.5 222,-46.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-31.3\">(None, 500)</text>\n<polyline fill=\"none\" points=\"222,-23.5 305,-23.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-8.3\">(None, 150)</text>\n</g>\n<!-- 139722447297744&#45;&gt;139722127240080 -->\n<g class=\"edge\" id=\"edge7\"><title>139722447297744-&gt;139722127240080</title>\n<path d=\"M273.419,-83.3664C254.575,-73.4551 232.192,-61.682 212.413,-51.2787\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"214.01,-48.1643 203.53,-46.6068 210.751,-54.3596 214.01,-48.1643\" stroke=\"black\"/>\n</g>\n<!-- 139722278204880 -->\n<g class=\"node\" id=\"node9\"><title>139722278204880</title>\n<polygon fill=\"none\" points=\"323,-0.5 323,-46.5 618,-46.5 618,-0.5 323,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"398.5\" y=\"-19.8\">submitter_output: Dense</text>\n<polyline fill=\"none\" points=\"474,-0.5 474,-46.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"501.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"474,-23.5 529,-23.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"501.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"529,-0.5 529,-46.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"573.5\" y=\"-31.3\">(None, 500)</text>\n<polyline fill=\"none\" points=\"529,-23.5 618,-23.5 \" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"573.5\" y=\"-8.3\">(None, 2037)</text>\n</g>\n<!-- 139722447297744&#45;&gt;139722278204880 -->\n<g class=\"edge\" id=\"edge8\"><title>139722447297744-&gt;139722278204880</title>\n<path d=\"M357.854,-83.3664C376.82,-73.4551 399.349,-61.682 419.257,-51.2787\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"420.955,-54.3404 428.197,-46.6068 417.713,-48.1364 420.955,-54.3404\" stroke=\"black\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "I_NVBbt9-h8U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Subreddit Multi-Label Classification"
      ]
    },
    {
      "metadata": {
        "id": "9Hy0PerZBvZ3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 132
            },
            {
              "item_id": 141
            },
            {
              "item_id": 142
            }
          ],
          "height": 878
        },
        "outputId": "50cb667e-447a-4835-e055-a110b4373826",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517793310264,
          "user_tz": 480,
          "elapsed": 293637,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# BOWEntitiesEncoded, Domain -> Subreddit \n",
        "# Multi-Label Classification\n",
        "\n",
        "current_learning_goal = 'MlbSubredditClassification'\n",
        "set_columns_for_goal()\n",
        "reddit_df = get_bq_data_for_goal()\n",
        "(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n",
        "\n",
        "\n",
        "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain'], outputs=['Subreddit'])\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting on bqjob_r5c366f40a8b69e72_0000016163837e53_4 ... (135s) Current status: DONE   "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Size of reddit set: 89311 records\n",
            "Size of train set: 70734 records\n",
            "Size of test set: 17683 records\n",
            "Using 150 unique values for subreddit\n",
            "Using 4141 unique values for domain\n",
            "Using 32207 unique values for BOWEntitiesEncoded\n",
            "Train on 63660 samples, validate on 7074 samples\n",
            "Epoch 1/2\n",
            " - 47s - loss: 0.0483 - acc: 0.9889 - top_k_categorical_accuracy: 0.5764 - val_loss: 0.0333 - val_acc: 0.9909 - val_top_k_categorical_accuracy: 0.7792\n",
            "Epoch 2/2\n",
            " - 47s - loss: 0.0293 - acc: 0.9916 - top_k_categorical_accuracy: 0.8248 - val_loss: 0.0289 - val_acc: 0.9915 - val_top_k_categorical_accuracy: 0.8343\n",
            "Test data loss: 0.029; top 1 accuracy: 0.992; top 5 accuracy: 0.831;\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG at 0x7f139319a610>"
            ],
            "image/svg+xml": "<svg height=\"483pt\" viewBox=\"0.00 0.00 311.50 483.00\" width=\"312pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-479 307.5,-479 307.5,4 -4,4\" stroke=\"none\"/>\n<!-- 139722151182608 -->\n<g class=\"node\" id=\"node1\"><title>139722151182608</title>\n<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 205,-474.5 205,-438.5 0,-438.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-452.8\">BOWEntitiesEncoded: InputLayer</text>\n</g>\n<!-- 139722151184528 -->\n<g class=\"node\" id=\"node2\"><title>139722151184528</title>\n<polygon fill=\"none\" points=\"22,-365.5 22,-401.5 183,-401.5 183,-365.5 22,-365.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-379.8\">embedding_9: Embedding</text>\n</g>\n<!-- 139722151182608&#45;&gt;139722151184528 -->\n<g class=\"edge\" id=\"edge1\"><title>139722151182608-&gt;139722151184528</title>\n<path d=\"M102.5,-438.313C102.5,-430.289 102.5,-420.547 102.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"106,-411.529 102.5,-401.529 99.0001,-411.529 106,-411.529\" stroke=\"black\"/>\n</g>\n<!-- 139722144747472 -->\n<g class=\"node\" id=\"node3\"><title>139722144747472</title>\n<polygon fill=\"none\" points=\"47.5,-292.5 47.5,-328.5 157.5,-328.5 157.5,-292.5 47.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-306.8\">flatten_9: Flatten</text>\n</g>\n<!-- 139722151184528&#45;&gt;139722144747472 -->\n<g class=\"edge\" id=\"edge2\"><title>139722151184528-&gt;139722144747472</title>\n<path d=\"M102.5,-365.313C102.5,-357.289 102.5,-347.547 102.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"106,-338.529 102.5,-328.529 99.0001,-338.529 106,-338.529\" stroke=\"black\"/>\n</g>\n<!-- 139722517172880 -->\n<g class=\"node\" id=\"node5\"><title>139722517172880</title>\n<polygon fill=\"none\" points=\"86.5,-219.5 86.5,-255.5 254.5,-255.5 254.5,-219.5 86.5,-219.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-233.8\">concatenate_6: Concatenate</text>\n</g>\n<!-- 139722144747472&#45;&gt;139722517172880 -->\n<g class=\"edge\" id=\"edge3\"><title>139722144747472-&gt;139722517172880</title>\n<path d=\"M118.961,-292.313C127.397,-283.505 137.817,-272.625 147.075,-262.958\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"149.802,-265.172 154.191,-255.529 144.746,-260.33 149.802,-265.172\" stroke=\"black\"/>\n</g>\n<!-- 139722430960592 -->\n<g class=\"node\" id=\"node4\"><title>139722430960592</title>\n<polygon fill=\"none\" points=\"175.5,-292.5 175.5,-328.5 303.5,-328.5 303.5,-292.5 175.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-306.8\">Domain: InputLayer</text>\n</g>\n<!-- 139722430960592&#45;&gt;139722517172880 -->\n<g class=\"edge\" id=\"edge4\"><title>139722430960592-&gt;139722517172880</title>\n<path d=\"M222.797,-292.313C214.237,-283.505 203.664,-272.625 194.269,-262.958\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"196.529,-260.261 187.049,-255.529 191.509,-265.14 196.529,-260.261\" stroke=\"black\"/>\n</g>\n<!-- 139722049038224 -->\n<g class=\"node\" id=\"node6\"><title>139722049038224</title>\n<polygon fill=\"none\" points=\"119.5,-146.5 119.5,-182.5 221.5,-182.5 221.5,-146.5 119.5,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-160.8\">dense_8: Dense</text>\n</g>\n<!-- 139722517172880&#45;&gt;139722049038224 -->\n<g class=\"edge\" id=\"edge5\"><title>139722517172880-&gt;139722049038224</title>\n<path d=\"M170.5,-219.313C170.5,-211.289 170.5,-201.547 170.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"174,-192.529 170.5,-182.529 167,-192.529 174,-192.529\" stroke=\"black\"/>\n</g>\n<!-- 139722148071056 -->\n<g class=\"node\" id=\"node7\"><title>139722148071056</title>\n<polygon fill=\"none\" points=\"108,-73.5 108,-109.5 233,-109.5 233,-73.5 108,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-87.8\">dropout_8: Dropout</text>\n</g>\n<!-- 139722049038224&#45;&gt;139722148071056 -->\n<g class=\"edge\" id=\"edge6\"><title>139722049038224-&gt;139722148071056</title>\n<path d=\"M170.5,-146.313C170.5,-138.289 170.5,-128.547 170.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"174,-119.529 170.5,-109.529 167,-119.529 174,-119.529\" stroke=\"black\"/>\n</g>\n<!-- 139722339917008 -->\n<g class=\"node\" id=\"node8\"><title>139722339917008</title>\n<polygon fill=\"none\" points=\"96,-0.5 96,-36.5 245,-36.5 245,-0.5 96,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-14.8\">subreddit_output: Dense</text>\n</g>\n<!-- 139722148071056&#45;&gt;139722339917008 -->\n<g class=\"edge\" id=\"edge7\"><title>139722148071056-&gt;139722339917008</title>\n<path d=\"M170.5,-73.3129C170.5,-65.2895 170.5,-55.5475 170.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"174,-46.5288 170.5,-36.5288 167,-46.5289 174,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "wBPVEnYv-sNH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-label accuracy metrics"
      ]
    },
    {
      "metadata": {
        "id": "GHuGRKKqZ69m",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            },
            {
              "item_id": 3
            }
          ],
          "height": 119
        },
        "outputId": "6d07f7d2-9924-492f-c881-83bcf1623036",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517798228986,
          "user_tz": 480,
          "elapsed": 122608,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Multi-label accuracy metrics\n",
        "\n",
        "(y_pred,acc_score,h_loss,r_loss) = eval_multilabel_metrics(model, x_test, y_true = y_test['subreddit_output'])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total correctly predicted: 4391 out of 17683 (absolute accuracy: 0.248)\n",
            "Multi-label accuracy score: 0.248\n",
            "Hamming loss: 0.014\n",
            "Label ranking loss: 0.031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/google/data/ro/teams/colab/mpm/tensorflow_notebook/tensorflow_notebook.mpm/versions/1-3c85c4ae_4104755e_b724a5d9_ac170c47_80ad5d7d/colab_notebook.par/google3/third_party/py/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "dOIH3fWs-ya-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-label accuracy by case type"
      ]
    },
    {
      "metadata": {
        "id": "Stb-Yk1e0KRm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "height": 1094
        },
        "outputId": "0dac766b-fa7b-4ee4-947e-d17978d3cf92",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517799528661,
          "user_tz": 480,
          "elapsed": 4274,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "(stats_num_samples, stats_num_cases, stats_num_cases_ratios, stats_by_casetype, example_by_casetype_bin) = calc_multilabel_accuracy_stats (\n",
        "    y_true = y_test['subreddit_output'], y_pred=y_pred)\n",
        "\n",
        "\n",
        "print('Samples by Number of Labels (2^x scale)')\n",
        "prettyprint_nparray(stats_num_samples,\n",
        "                    col_headers=['Num Labels'],\n",
        "                    row_headers=gen_label_bin_headers(stats_num_samples.shape[0]-1))\n",
        "\n",
        "print('\\nSamples by Number of Labels X Case Type')\n",
        "prettyprint_nparray(stats_num_cases,\n",
        "                    col_headers=CASE_TYPE_HEADERS,\n",
        "                    row_headers=gen_label_bin_headers(stats_num_cases.shape[0]-1))\n",
        "                     \n",
        "print('\\nSamples by Number of Labels X Case Type (Ratios add up to 100%)')\n",
        "prettyprint_nparray(stats_num_cases_ratios,\n",
        "                    col_headers=CASE_TYPE_HEADERS,\n",
        "                    row_headers=gen_label_bin_headers(stats_num_cases_ratios.shape[0]-1))\n",
        "\n",
        "print('\\nSamples by Case Type (Ratios add up to 100%)')\n",
        "prettyprint_nparray(stats_by_casetype,\n",
        "                    col_headers=['Ratio'],\n",
        "                    row_headers=CASE_TYPE_HEADERS)\n",
        "\n",
        "print('\\nSample indeces by Number of Labels X Case Type')\n",
        "prettyprint_nparray(example_by_casetype_bin,\n",
        "                    col_headers=CASE_TYPE_HEADERS,\n",
        "                    row_headers=gen_label_bin_headers(example_by_casetype_bin.shape[0]-1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples by Number of Labels (2^x scale)\n",
            "        Num Labels\n",
            "1            11940\n",
            "2             3087\n",
            "3..4          1895\n",
            "5..8           648\n",
            "9..16          108\n",
            "17..32           5\n",
            "\n",
            "Samples by Number of Labels X Case Type\n",
            "        100% TP+TN  50%+ TP  1-49% TP  0% TP\n",
            "1             3939     4229         0   3772\n",
            "2              346     1999         0    742\n",
            "3..4           102      961       544    288\n",
            "5..8             4      263       321     60\n",
            "9..16            0       23        82      3\n",
            "17..32           0        0         5      0\n",
            "\n",
            "Samples by Number of Labels X Case Type (Ratios add up to 100%)\n",
            "        100% TP+TN  50%+ TP  1-49% TP  0% TP\n",
            "1            0.223    0.239     0.000  0.213\n",
            "2            0.020    0.113     0.000  0.042\n",
            "3..4         0.006    0.054     0.031  0.016\n",
            "5..8         0.000    0.015     0.018  0.003\n",
            "9..16        0.000    0.001     0.005  0.000\n",
            "17..32       0.000    0.000     0.000  0.000\n",
            "\n",
            "Samples by Case Type (Ratios add up to 100%)\n",
            "            Ratio\n",
            "100% TP+TN  0.249\n",
            "50%+ TP     0.422\n",
            "1-49% TP    0.054\n",
            "0% TP       0.274\n",
            "\n",
            "Sample indeces by Number of Labels X Case Type\n",
            "        100% TP+TN  50%+ TP  1-49% TP  0% TP\n",
            "1                0        5        -1     11\n",
            "2               42        3        -1      8\n",
            "3..4           335       32        37     52\n",
            "5..8          8945      105        56    890\n",
            "9..16           -1     1104       437   5313\n",
            "17..32          -1       -1       216     -1\n",
            "Test record index #4482 example of 100% TP+TN\n",
            "News Urls: https://www.theguardian.com/environment/2017/jun/05/public-lands-uranium-mining-arizona-grand-canyon\n",
            "Reddit Post Urls: https://www.reddit.com/r/politics/comments/6fdjjr/ https://www.reddit.com/r/worldnewshub/comments/6fe7fh/ https://www.reddit.com/r/worldnews/comments/6fe2fc/ https://www.reddit.com/r/EnoughTrumpSpam/comments/6fj967/ https://www.reddit.com/r/esist/comments/6fj97z/ https://www.reddit.com/r/MarchAgainstTrump/comments/6fj94k/ https://www.reddit.com/r/DonaldTrumpWhiteHouse/comments/6fdpu2/ https://www.reddit.com/r/RedditSample/comments/6fdbk6/ https://www.reddit.com/r/environment/comments/6fe2do/ https://www.reddit.com/r/news/comments/6fe2io/ https://www.reddit.com/r/EcoInternet/comments/6fd9qo/ https://www.reddit.com/r/progressive/comments/6fe2la/ https://www.reddit.com/r/SandersForPresident/comments/6fe2mo/\n",
            "\n",
            "Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [theguardian.com]\n",
            "Predicted classes: ecointernet,environment\n",
            "True/Actual classes: politics,worldnewshub,news,worldnews,ecointernet,donaldtrumpwhitehouse,redditsample,enoughtrumpspam,environment,esist,marchagainsttrump,sandersforpresident,progressive\n",
            "\n",
            "Test record index #10 example of 50%+ TP\n",
            "News Urls: http://www.androidauthority.com/huawei-overtakes-apple-smartphone-shipments-779231/\n",
            "Reddit Post Urls: https://www.reddit.com/r/Android/comments/6grsnj/\n",
            "\n",
            "Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [androidauthority.com]\n",
            "Predicted classes: technology,mobilityreport,futurology,android\n",
            "True/Actual classes: android\n",
            "\n",
            "Test record index #437 example of 1-49%+ TP\n",
            "News Urls: https://www.rt.com/news/392080-eu-legal-action-refugees/\n",
            "Reddit Post Urls: https://www.reddit.com/r/The_Donald/comments/6h0zoc/ https://www.reddit.com/r/worldnews/comments/6h1yd0/ https://www.reddit.com/r/metacanada/comments/6h1blt/ https://www.reddit.com/r/Conservative/comments/6h0fzi/ https://www.reddit.com/r/worldnewshub/comments/6h091t/ https://www.reddit.com/r/worldnews/comments/6h03w0/ https://www.reddit.com/r/The_Donald/comments/6h2wkn/ https://www.reddit.com/r/The_Donald/comments/6h4k6o/ https://www.reddit.com/r/uncensorednews/comments/6h2w93/ https://www.reddit.com/r/joinordietrump/comments/6h1h5g/ https://www.reddit.com/r/PublicBelief/comments/6h21e1/ https://www.reddit.com/r/The_Europe/comments/6h2wrb/ https://www.reddit.com/r/europe/comments/6h04os/ https://www.reddit.com/r/JustBadNews/comments/6h0cx0/\n",
            "\n",
            "Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [rt.com]\n",
            "Predicted classes: worldnewshub,worldnews\n",
            "True/Actual classes: worldnewshub,worldnews,the_donald,europe,conservative,publicbelief,uncensorednews,justbadnews,metacanada,joinordietrump,the_europe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KXeYVumn-82A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-Label prediction examples"
      ]
    },
    {
      "metadata": {
        "id": "H25yychh6AFn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 281
        },
        "outputId": "7cb73cfa-1be3-4362-cbf8-e0bce85e456a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517802872789,
          "user_tz": 480,
          "elapsed": 693,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print_prediction_and_input(8945, notes='example of 100% TP+TN',\n",
        "                           y_true=y_test['subreddit_output'], y_pred=y_pred, classes_dict=subreddit_classes)  \n",
        "\n",
        "print('')\n",
        "\n",
        "print_prediction_and_input(1104, notes='example of 50%+ TP',\n",
        "                           y_true=y_test['subreddit_output'], y_pred=y_pred, classes_dict=subreddit_classes)  \n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test record index #8945 example of 100% TP+TN\n",
            "News Urls: https://www.engadget.com/2017/06/20/intel-cancels-galileo-joule-edison/\n",
            "Reddit Post Urls: https://www.reddit.com/r/Raytheon/comments/6iisrt/ https://www.reddit.com/r/Engadget/comments/6ihqq1/ https://www.reddit.com/r/TheColorIsOrange/comments/6ihqjo/ https://www.reddit.com/r/SkydTech/comments/6ihr0e/ https://www.reddit.com/r/technology/comments/6iiqgx/\n",
            "\n",
            "Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [engadget.com]\n",
            "Predicted classes: raytheon,technology,skydtech,thecolorisorange,engadget\n",
            "True/Actual classes: raytheon,technology,skydtech,thecolorisorange,engadget\n",
            "\n",
            "Test record index #1104 example of 50%+ TP\n",
            "News Urls: http://www.bbc.co.uk/news/world-asia-40118690\n",
            "Reddit Post Urls: https://www.reddit.com/r/worldnewshub/comments/6elmoz/ https://www.reddit.com/r/UpliftingNews/comments/6eluxc/ https://www.reddit.com/r/TheColorIsRed/comments/6eljyd/ https://www.reddit.com/r/DonaldTrumpWhiteHouse/comments/6em67s/ https://www.reddit.com/r/politics/comments/6en6cz/ https://www.reddit.com/r/bbcnewsuk/comments/6elky2/ https://www.reddit.com/r/EcoInternet/comments/6en7ig/ https://www.reddit.com/r/nottheonion/comments/6eljqe/ https://www.reddit.com/r/esist/comments/6en6fg/ https://www.reddit.com/r/worldnews/comments/6eljrb/\n",
            "\n",
            "Prediction input:  BOWEntitiesEncoded [Decode function not implemented] Domain [bbc.co.uk]\n",
            "Predicted classes: worldnewshub,worldnews,ecointernet,bbcnewsuk,thecolorisred\n",
            "True/Actual classes: politics,worldnewshub,worldnews,ecointernet,donaldtrumpwhitehouse,nottheonion,bbcnewsuk,thecolorisred,esist,upliftingnews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-OE5bQo8_NFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adding Submitter as Predictor of Subreddit"
      ]
    },
    {
      "metadata": {
        "id": "9o2ObHjfIZqt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 107
            },
            {
              "item_id": 116
            },
            {
              "item_id": 117
            }
          ],
          "height": 894
        },
        "outputId": "264d8792-a8a9-4450-b3bc-7f55efd463f5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517804402796,
          "user_tz": 480,
          "elapsed": 396513,
          "user": {
            "displayName": "Sergei Sokolenko",
            "photoUrl": "//lh6.googleusercontent.com/-iyAOAY88u7I/AAAAAAAAAAI/AAAAAAAAABA/9ECequ778Ss/s50-c-k-no/photo.jpg",
            "userId": "106652311097239389877"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# BOWEntitiesEncoded, Domain, RedditSubmitter -> Subreddit\n",
        "# Single-label classification\n",
        "\n",
        "current_learning_goal = 'SubredditClassification'\n",
        "set_columns_for_goal()\n",
        "reddit_df = get_bq_data_for_goal()\n",
        "(training_features, training_labels,validation_features, validation_labels) = create_train_test_features_labels()\n",
        "\n",
        "(model, history, x_train, x_test, y_train, y_test) = compile_and_fit_model(inputs = ['BOWEntitiesEncoded','Domain','RedditSubmitter'], outputs=['Subreddit'])\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting on bqjob_r3b059bd1e87eeee4_00000161642b2e33_5 ... (108s) Current status: DONE   "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Size of reddit set: 154931 records\n",
            "Size of train set: 122705 records\n",
            "Size of test set: 30676 records\n",
            "Using 150 unique values for subreddit\n",
            "Using 4232 unique values for domain\n",
            "Using 2037 unique values for submitter\n",
            "Using 32376 unique values for BOWEntitiesEncoded\n",
            "Train on 110434 samples, validate on 12271 samples\n",
            "Epoch 1/2\n",
            " - 101s - loss: 1.5830 - acc: 0.6287 - top_k_categorical_accuracy: 0.8180 - val_loss: 0.9823 - val_acc: 0.7329 - val_top_k_categorical_accuracy: 0.9122\n",
            "Epoch 2/2\n",
            " - 101s - loss: 0.8024 - acc: 0.7693 - top_k_categorical_accuracy: 0.9421 - val_loss: 0.9652 - val_acc: 0.7300 - val_top_k_categorical_accuracy: 0.9200\n",
            "Test data loss: 0.987; top 1 accuracy: 0.723; top 5 accuracy: 0.917;\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG at 0x7f13947f9d50>"
            ],
            "image/svg+xml": "<svg height=\"483pt\" viewBox=\"0.00 0.00 506.50 483.00\" width=\"507pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n<title>G</title>\n<polygon fill=\"white\" points=\"-4,4 -4,-479 502.5,-479 502.5,4 -4,4\" stroke=\"none\"/>\n<!-- 139722010859152 -->\n<g class=\"node\" id=\"node1\"><title>139722010859152</title>\n<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 205,-474.5 205,-438.5 0,-438.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-452.8\">BOWEntitiesEncoded: InputLayer</text>\n</g>\n<!-- 139716529109648 -->\n<g class=\"node\" id=\"node2\"><title>139716529109648</title>\n<polygon fill=\"none\" points=\"18.5,-365.5 18.5,-401.5 186.5,-401.5 186.5,-365.5 18.5,-365.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-379.8\">embedding_10: Embedding</text>\n</g>\n<!-- 139722010859152&#45;&gt;139716529109648 -->\n<g class=\"edge\" id=\"edge1\"><title>139722010859152-&gt;139716529109648</title>\n<path d=\"M102.5,-438.313C102.5,-430.289 102.5,-420.547 102.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"106,-411.529 102.5,-401.529 99.0001,-411.529 106,-411.529\" stroke=\"black\"/>\n</g>\n<!-- 139722328310096 -->\n<g class=\"node\" id=\"node3\"><title>139722328310096</title>\n<polygon fill=\"none\" points=\"44.5,-292.5 44.5,-328.5 160.5,-328.5 160.5,-292.5 44.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-306.8\">flatten_10: Flatten</text>\n</g>\n<!-- 139716529109648&#45;&gt;139722328310096 -->\n<g class=\"edge\" id=\"edge2\"><title>139716529109648-&gt;139722328310096</title>\n<path d=\"M102.5,-365.313C102.5,-357.289 102.5,-347.547 102.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"106,-338.529 102.5,-328.529 99.0001,-338.529 106,-338.529\" stroke=\"black\"/>\n</g>\n<!-- 139722264038288 -->\n<g class=\"node\" id=\"node6\"><title>139722264038288</title>\n<polygon fill=\"none\" points=\"158.5,-219.5 158.5,-255.5 326.5,-255.5 326.5,-219.5 158.5,-219.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-233.8\">concatenate_7: Concatenate</text>\n</g>\n<!-- 139722328310096&#45;&gt;139722264038288 -->\n<g class=\"edge\" id=\"edge3\"><title>139722328310096-&gt;139722264038288</title>\n<path d=\"M136.033,-292.494C155.114,-282.817 179.211,-270.597 199.652,-260.23\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"201.419,-263.258 208.755,-255.614 198.253,-257.015 201.419,-263.258\" stroke=\"black\"/>\n</g>\n<!-- 139716548804304 -->\n<g class=\"node\" id=\"node4\"><title>139716548804304</title>\n<polygon fill=\"none\" points=\"178.5,-292.5 178.5,-328.5 306.5,-328.5 306.5,-292.5 178.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-306.8\">Domain: InputLayer</text>\n</g>\n<!-- 139716548804304&#45;&gt;139722264038288 -->\n<g class=\"edge\" id=\"edge4\"><title>139716548804304-&gt;139722264038288</title>\n<path d=\"M242.5,-292.313C242.5,-284.289 242.5,-274.547 242.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"246,-265.529 242.5,-255.529 239,-265.529 246,-265.529\" stroke=\"black\"/>\n</g>\n<!-- 139721980205392 -->\n<g class=\"node\" id=\"node5\"><title>139721980205392</title>\n<polygon fill=\"none\" points=\"324.5,-292.5 324.5,-328.5 498.5,-328.5 498.5,-292.5 324.5,-292.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"411.5\" y=\"-306.8\">RedditSubmitter: InputLayer</text>\n</g>\n<!-- 139721980205392&#45;&gt;139722264038288 -->\n<g class=\"edge\" id=\"edge5\"><title>139721980205392-&gt;139722264038288</title>\n<path d=\"M371.02,-292.494C347.468,-282.599 317.586,-270.045 292.564,-259.533\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"293.81,-256.26 283.235,-255.614 291.099,-262.714 293.81,-256.26\" stroke=\"black\"/>\n</g>\n<!-- 139722308928976 -->\n<g class=\"node\" id=\"node7\"><title>139722308928976</title>\n<polygon fill=\"none\" points=\"191.5,-146.5 191.5,-182.5 293.5,-182.5 293.5,-146.5 191.5,-146.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-160.8\">dense_9: Dense</text>\n</g>\n<!-- 139722264038288&#45;&gt;139722308928976 -->\n<g class=\"edge\" id=\"edge6\"><title>139722264038288-&gt;139722308928976</title>\n<path d=\"M242.5,-219.313C242.5,-211.289 242.5,-201.547 242.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"246,-192.529 242.5,-182.529 239,-192.529 246,-192.529\" stroke=\"black\"/>\n</g>\n<!-- 139722151066960 -->\n<g class=\"node\" id=\"node8\"><title>139722151066960</title>\n<polygon fill=\"none\" points=\"180,-73.5 180,-109.5 305,-109.5 305,-73.5 180,-73.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-87.8\">dropout_9: Dropout</text>\n</g>\n<!-- 139722308928976&#45;&gt;139722151066960 -->\n<g class=\"edge\" id=\"edge7\"><title>139722308928976-&gt;139722151066960</title>\n<path d=\"M242.5,-146.313C242.5,-138.289 242.5,-128.547 242.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"246,-119.529 242.5,-109.529 239,-119.529 246,-119.529\" stroke=\"black\"/>\n</g>\n<!-- 139722151065488 -->\n<g class=\"node\" id=\"node9\"><title>139722151065488</title>\n<polygon fill=\"none\" points=\"168,-0.5 168,-36.5 317,-36.5 317,-0.5 168,-0.5\" stroke=\"black\"/>\n<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-14.8\">subreddit_output: Dense</text>\n</g>\n<!-- 139722151066960&#45;&gt;139722151065488 -->\n<g class=\"edge\" id=\"edge8\"><title>139722151066960-&gt;139722151065488</title>\n<path d=\"M242.5,-73.3129C242.5,-65.2895 242.5,-55.5475 242.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n<polygon fill=\"black\" points=\"246,-46.5288 242.5,-36.5288 239,-46.5289 246,-46.5288\" stroke=\"black\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}